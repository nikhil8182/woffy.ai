import {
  Component,
  Deferred,
  FirebaseError,
  Logger,
  _getProvider,
  _isFirebaseServerApp,
  _registerComponent,
  getApp,
  getModularInstance,
  registerVersion
} from "./chunk-X522QPFF.js";
import "./chunk-G3PMV62Z.js";

// node_modules/@firebase/ai/dist/esm/index.esm.js
var name = "@firebase/ai";
var version = "2.6.0";
var AI_TYPE = "AI";
var DEFAULT_LOCATION = "us-central1";
var DEFAULT_DOMAIN = "firebasevertexai.googleapis.com";
var DEFAULT_API_VERSION = "v1beta";
var PACKAGE_VERSION = version;
var LANGUAGE_TAG = "gl-js";
var DEFAULT_FETCH_TIMEOUT_MS = 180 * 1e3;
var DEFAULT_HYBRID_IN_CLOUD_MODEL = "gemini-2.0-flash-lite";
var AIError = class _AIError extends FirebaseError {
  /**
   * Constructs a new instance of the `AIError` class.
   *
   * @param code - The error code from {@link (AIErrorCode:type)}.
   * @param message - A human-readable message describing the error.
   * @param customErrorData - Optional error data.
   */
  constructor(code, message, customErrorData) {
    const service = AI_TYPE;
    const fullCode = `${service}/${code}`;
    const fullMessage = `${service}: ${message} (${fullCode})`;
    super(code, fullMessage);
    this.code = code;
    this.customErrorData = customErrorData;
    if (Error.captureStackTrace) {
      Error.captureStackTrace(this, _AIError);
    }
    Object.setPrototypeOf(this, _AIError.prototype);
    this.toString = () => fullMessage;
  }
};
var POSSIBLE_ROLES = ["user", "model", "function", "system"];
var HarmCategory = {
  HARM_CATEGORY_HATE_SPEECH: "HARM_CATEGORY_HATE_SPEECH",
  HARM_CATEGORY_SEXUALLY_EXPLICIT: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
  HARM_CATEGORY_HARASSMENT: "HARM_CATEGORY_HARASSMENT",
  HARM_CATEGORY_DANGEROUS_CONTENT: "HARM_CATEGORY_DANGEROUS_CONTENT"
};
var HarmBlockThreshold = {
  /**
   * Content with `NEGLIGIBLE` will be allowed.
   */
  BLOCK_LOW_AND_ABOVE: "BLOCK_LOW_AND_ABOVE",
  /**
   * Content with `NEGLIGIBLE` and `LOW` will be allowed.
   */
  BLOCK_MEDIUM_AND_ABOVE: "BLOCK_MEDIUM_AND_ABOVE",
  /**
   * Content with `NEGLIGIBLE`, `LOW`, and `MEDIUM` will be allowed.
   */
  BLOCK_ONLY_HIGH: "BLOCK_ONLY_HIGH",
  /**
   * All content will be allowed.
   */
  BLOCK_NONE: "BLOCK_NONE",
  /**
   * All content will be allowed. This is the same as `BLOCK_NONE`, but the metadata corresponding
   * to the {@link (HarmCategory:type)} will not be present in the response.
   */
  OFF: "OFF"
};
var HarmBlockMethod = {
  /**
   * The harm block method uses both probability and severity scores.
   */
  SEVERITY: "SEVERITY",
  /**
   * The harm block method uses the probability score.
   */
  PROBABILITY: "PROBABILITY"
};
var HarmProbability = {
  /**
   * Content has a negligible chance of being unsafe.
   */
  NEGLIGIBLE: "NEGLIGIBLE",
  /**
   * Content has a low chance of being unsafe.
   */
  LOW: "LOW",
  /**
   * Content has a medium chance of being unsafe.
   */
  MEDIUM: "MEDIUM",
  /**
   * Content has a high chance of being unsafe.
   */
  HIGH: "HIGH"
};
var HarmSeverity = {
  /**
   * Negligible level of harm severity.
   */
  HARM_SEVERITY_NEGLIGIBLE: "HARM_SEVERITY_NEGLIGIBLE",
  /**
   * Low level of harm severity.
   */
  HARM_SEVERITY_LOW: "HARM_SEVERITY_LOW",
  /**
   * Medium level of harm severity.
   */
  HARM_SEVERITY_MEDIUM: "HARM_SEVERITY_MEDIUM",
  /**
   * High level of harm severity.
   */
  HARM_SEVERITY_HIGH: "HARM_SEVERITY_HIGH",
  /**
   * Harm severity is not supported.
   *
   * @remarks
   * The GoogleAI backend does not support `HarmSeverity`, so this value is used as a fallback.
   */
  HARM_SEVERITY_UNSUPPORTED: "HARM_SEVERITY_UNSUPPORTED"
};
var BlockReason = {
  /**
   * Content was blocked by safety settings.
   */
  SAFETY: "SAFETY",
  /**
   * Content was blocked, but the reason is uncategorized.
   */
  OTHER: "OTHER",
  /**
   * Content was blocked because it contained terms from the terminology blocklist.
   */
  BLOCKLIST: "BLOCKLIST",
  /**
   * Content was blocked due to prohibited content.
   */
  PROHIBITED_CONTENT: "PROHIBITED_CONTENT"
};
var FinishReason = {
  /**
   * Natural stop point of the model or provided stop sequence.
   */
  STOP: "STOP",
  /**
   * The maximum number of tokens as specified in the request was reached.
   */
  MAX_TOKENS: "MAX_TOKENS",
  /**
   * The candidate content was flagged for safety reasons.
   */
  SAFETY: "SAFETY",
  /**
   * The candidate content was flagged for recitation reasons.
   */
  RECITATION: "RECITATION",
  /**
   * Unknown reason.
   */
  OTHER: "OTHER",
  /**
   * The candidate content contained forbidden terms.
   */
  BLOCKLIST: "BLOCKLIST",
  /**
   * The candidate content potentially contained prohibited content.
   */
  PROHIBITED_CONTENT: "PROHIBITED_CONTENT",
  /**
   * The candidate content potentially contained Sensitive Personally Identifiable Information (SPII).
   */
  SPII: "SPII",
  /**
   * The function call generated by the model was invalid.
   */
  MALFORMED_FUNCTION_CALL: "MALFORMED_FUNCTION_CALL"
};
var FunctionCallingMode = {
  /**
   * Default model behavior; model decides to predict either a function call
   * or a natural language response.
   */
  AUTO: "AUTO",
  /**
   * Model is constrained to always predicting a function call only.
   * If `allowed_function_names` is set, the predicted function call will be
   * limited to any one of `allowed_function_names`, else the predicted
   * function call will be any one of the provided `function_declarations`.
   */
  ANY: "ANY",
  /**
   * Model will not predict any function call. Model behavior is same as when
   * not passing any function declarations.
   */
  NONE: "NONE"
};
var Modality = {
  /**
   * Unspecified modality.
   */
  MODALITY_UNSPECIFIED: "MODALITY_UNSPECIFIED",
  /**
   * Plain text.
   */
  TEXT: "TEXT",
  /**
   * Image.
   */
  IMAGE: "IMAGE",
  /**
   * Video.
   */
  VIDEO: "VIDEO",
  /**
   * Audio.
   */
  AUDIO: "AUDIO",
  /**
   * Document (for example, PDF).
   */
  DOCUMENT: "DOCUMENT"
};
var ResponseModality = {
  /**
   * Text.
   * @beta
   */
  TEXT: "TEXT",
  /**
   * Image.
   * @beta
   */
  IMAGE: "IMAGE",
  /**
   * Audio.
   * @beta
   */
  AUDIO: "AUDIO"
};
var InferenceMode = {
  "PREFER_ON_DEVICE": "prefer_on_device",
  "ONLY_ON_DEVICE": "only_on_device",
  "ONLY_IN_CLOUD": "only_in_cloud",
  "PREFER_IN_CLOUD": "prefer_in_cloud"
};
var InferenceSource = {
  "ON_DEVICE": "on_device",
  "IN_CLOUD": "in_cloud"
};
var Outcome = {
  UNSPECIFIED: "OUTCOME_UNSPECIFIED",
  OK: "OUTCOME_OK",
  FAILED: "OUTCOME_FAILED",
  DEADLINE_EXCEEDED: "OUTCOME_DEADLINE_EXCEEDED"
};
var Language = {
  UNSPECIFIED: "LANGUAGE_UNSPECIFIED",
  PYTHON: "PYTHON"
};
var URLRetrievalStatus = {
  /**
   * Unspecified retrieval status.
   */
  URL_RETRIEVAL_STATUS_UNSPECIFIED: "URL_RETRIEVAL_STATUS_UNSPECIFIED",
  /**
   * The URL retrieval was successful.
   */
  URL_RETRIEVAL_STATUS_SUCCESS: "URL_RETRIEVAL_STATUS_SUCCESS",
  /**
   * The URL retrieval failed.
   */
  URL_RETRIEVAL_STATUS_ERROR: "URL_RETRIEVAL_STATUS_ERROR",
  /**
   * The URL retrieval failed because the content is behind a paywall.
   */
  URL_RETRIEVAL_STATUS_PAYWALL: "URL_RETRIEVAL_STATUS_PAYWALL",
  /**
   * The URL retrieval failed because the content is unsafe.
   */
  URL_RETRIEVAL_STATUS_UNSAFE: "URL_RETRIEVAL_STATUS_UNSAFE"
};
var LiveResponseType = {
  SERVER_CONTENT: "serverContent",
  TOOL_CALL: "toolCall",
  TOOL_CALL_CANCELLATION: "toolCallCancellation"
};
var AIErrorCode = {
  /** A generic error occurred. */
  ERROR: "error",
  /** An error occurred in a request. */
  REQUEST_ERROR: "request-error",
  /** An error occurred in a response. */
  RESPONSE_ERROR: "response-error",
  /** An error occurred while performing a fetch. */
  FETCH_ERROR: "fetch-error",
  /** An error occurred because an operation was attempted on a closed session. */
  SESSION_CLOSED: "session-closed",
  /** An error associated with a Content object.  */
  INVALID_CONTENT: "invalid-content",
  /** An error due to the Firebase API not being enabled in the Console. */
  API_NOT_ENABLED: "api-not-enabled",
  /** An error due to invalid Schema input.  */
  INVALID_SCHEMA: "invalid-schema",
  /** An error occurred due to a missing Firebase API key. */
  NO_API_KEY: "no-api-key",
  /** An error occurred due to a missing Firebase app ID. */
  NO_APP_ID: "no-app-id",
  /** An error occurred due to a model name not being specified during initialization. */
  NO_MODEL: "no-model",
  /** An error occurred due to a missing project ID. */
  NO_PROJECT_ID: "no-project-id",
  /** An error occurred while parsing. */
  PARSE_FAILED: "parse-failed",
  /** An error occurred due an attempt to use an unsupported feature. */
  UNSUPPORTED: "unsupported"
};
var SchemaType = {
  /** String type. */
  STRING: "string",
  /** Number type. */
  NUMBER: "number",
  /** Integer type. */
  INTEGER: "integer",
  /** Boolean type. */
  BOOLEAN: "boolean",
  /** Array type. */
  ARRAY: "array",
  /** Object type. */
  OBJECT: "object"
};
var ImagenSafetyFilterLevel = {
  /**
   * The most aggressive filtering level; most strict blocking.
   */
  BLOCK_LOW_AND_ABOVE: "block_low_and_above",
  /**
   * Blocks some sensitive prompts and responses.
   */
  BLOCK_MEDIUM_AND_ABOVE: "block_medium_and_above",
  /**
   * Blocks few sensitive prompts and responses.
   */
  BLOCK_ONLY_HIGH: "block_only_high",
  /**
   * The least aggressive filtering level; blocks very few sensitive prompts and responses.
   *
   * Access to this feature is restricted and may require your case to be reviewed and approved by
   * Cloud support.
   */
  BLOCK_NONE: "block_none"
};
var ImagenPersonFilterLevel = {
  /**
   * Disallow generation of images containing people or faces; images of people are filtered out.
   */
  BLOCK_ALL: "dont_allow",
  /**
   * Allow generation of images containing adults only; images of children are filtered out.
   *
   * Generation of images containing people or faces may require your use case to be
   * reviewed and approved by Cloud support; see the {@link https://cloud.google.com/vertex-ai/generative-ai/docs/image/responsible-ai-imagen#person-face-gen | Responsible AI and usage guidelines}
   * for more details.
   */
  ALLOW_ADULT: "allow_adult",
  /**
   * Allow generation of images containing adults only; images of children are filtered out.
   *
   * Generation of images containing people or faces may require your use case to be
   * reviewed and approved by Cloud support; see the {@link https://cloud.google.com/vertex-ai/generative-ai/docs/image/responsible-ai-imagen#person-face-gen | Responsible AI and usage guidelines}
   * for more details.
   */
  ALLOW_ALL: "allow_all"
};
var ImagenAspectRatio = {
  /**
   * Square (1:1) aspect ratio.
   */
  "SQUARE": "1:1",
  /**
   * Landscape (3:4) aspect ratio.
   */
  "LANDSCAPE_3x4": "3:4",
  /**
   * Portrait (4:3) aspect ratio.
   */
  "PORTRAIT_4x3": "4:3",
  /**
   * Landscape (16:9) aspect ratio.
   */
  "LANDSCAPE_16x9": "16:9",
  /**
   * Portrait (9:16) aspect ratio.
   */
  "PORTRAIT_9x16": "9:16"
};
var BackendType = {
  /**
   * Identifies the backend service for the Vertex AI Gemini API provided through Google Cloud.
   * Use this constant when creating a {@link VertexAIBackend} configuration.
   */
  VERTEX_AI: "VERTEX_AI",
  /**
   * Identifies the backend service for the Gemini Developer API ({@link https://ai.google/ | Google AI}).
   * Use this constant when creating a {@link GoogleAIBackend} configuration.
   */
  GOOGLE_AI: "GOOGLE_AI"
};
var Backend = class {
  /**
   * Protected constructor for use by subclasses.
   * @param type - The backend type.
   */
  constructor(type) {
    this.backendType = type;
  }
};
var GoogleAIBackend = class extends Backend {
  /**
   * Creates a configuration object for the Gemini Developer API backend.
   */
  constructor() {
    super(BackendType.GOOGLE_AI);
  }
  /**
   * @internal
   */
  _getModelPath(project, model) {
    return `/${DEFAULT_API_VERSION}/projects/${project}/${model}`;
  }
  /**
   * @internal
   */
  _getTemplatePath(project, templateId) {
    return `/${DEFAULT_API_VERSION}/projects/${project}/templates/${templateId}`;
  }
};
var VertexAIBackend = class extends Backend {
  /**
   * Creates a configuration object for the Vertex AI backend.
   *
   * @param location - The region identifier, defaulting to `us-central1`;
   * see {@link https://firebase.google.com/docs/vertex-ai/locations#available-locations | Vertex AI locations}
   * for a list of supported locations.
   */
  constructor(location = DEFAULT_LOCATION) {
    super(BackendType.VERTEX_AI);
    if (!location) {
      this.location = DEFAULT_LOCATION;
    } else {
      this.location = location;
    }
  }
  /**
   * @internal
   */
  _getModelPath(project, model) {
    return `/${DEFAULT_API_VERSION}/projects/${project}/locations/${this.location}/${model}`;
  }
  /**
   * @internal
   */
  _getTemplatePath(project, templateId) {
    return `/${DEFAULT_API_VERSION}/projects/${project}/locations/${this.location}/templates/${templateId}`;
  }
};
function encodeInstanceIdentifier(backend) {
  if (backend instanceof GoogleAIBackend) {
    return `${AI_TYPE}/googleai`;
  } else if (backend instanceof VertexAIBackend) {
    return `${AI_TYPE}/vertexai/${backend.location}`;
  } else {
    throw new AIError(AIErrorCode.ERROR, `Invalid backend: ${JSON.stringify(backend.backendType)}`);
  }
}
function decodeInstanceIdentifier(instanceIdentifier) {
  const identifierParts = instanceIdentifier.split("/");
  if (identifierParts[0] !== AI_TYPE) {
    throw new AIError(AIErrorCode.ERROR, `Invalid instance identifier, unknown prefix '${identifierParts[0]}'`);
  }
  const backendType = identifierParts[1];
  switch (backendType) {
    case "vertexai":
      const location = identifierParts[2];
      if (!location) {
        throw new AIError(AIErrorCode.ERROR, `Invalid instance identifier, unknown location '${instanceIdentifier}'`);
      }
      return new VertexAIBackend(location);
    case "googleai":
      return new GoogleAIBackend();
    default:
      throw new AIError(AIErrorCode.ERROR, `Invalid instance identifier string: '${instanceIdentifier}'`);
  }
}
var logger = new Logger("@firebase/vertexai");
var Availability;
(function(Availability2) {
  Availability2["UNAVAILABLE"] = "unavailable";
  Availability2["DOWNLOADABLE"] = "downloadable";
  Availability2["DOWNLOADING"] = "downloading";
  Availability2["AVAILABLE"] = "available";
})(Availability || (Availability = {}));
var defaultExpectedInputs = [{ type: "image" }];
var ChromeAdapterImpl = class _ChromeAdapterImpl {
  constructor(languageModelProvider, mode, onDeviceParams) {
    this.languageModelProvider = languageModelProvider;
    this.mode = mode;
    this.isDownloading = false;
    this.onDeviceParams = {
      createOptions: {
        expectedInputs: defaultExpectedInputs
      }
    };
    if (onDeviceParams) {
      this.onDeviceParams = onDeviceParams;
      if (!this.onDeviceParams.createOptions) {
        this.onDeviceParams.createOptions = {
          expectedInputs: defaultExpectedInputs
        };
      } else if (!this.onDeviceParams.createOptions.expectedInputs) {
        this.onDeviceParams.createOptions.expectedInputs = defaultExpectedInputs;
      }
    }
  }
  /**
   * Checks if a given request can be made on-device.
   *
   * Encapsulates a few concerns:
   *   the mode
   *   API existence
   *   prompt formatting
   *   model availability, including triggering download if necessary
   *
   *
   * Pros: callers needn't be concerned with details of on-device availability.</p>
   * Cons: this method spans a few concerns and splits request validation from usage.
   * If instance variables weren't already part of the API, we could consider a better
   * separation of concerns.
   */
  async isAvailable(request) {
    if (!this.mode) {
      logger.debug(`On-device inference unavailable because mode is undefined.`);
      return false;
    }
    if (this.mode === InferenceMode.ONLY_IN_CLOUD) {
      logger.debug(`On-device inference unavailable because mode is "only_in_cloud".`);
      return false;
    }
    const availability = await this.downloadIfAvailable();
    if (this.mode === InferenceMode.ONLY_ON_DEVICE) {
      if (availability === Availability.UNAVAILABLE) {
        throw new AIError(AIErrorCode.API_NOT_ENABLED, "Local LanguageModel API not available in this environment.");
      } else if (availability === Availability.DOWNLOADABLE || availability === Availability.DOWNLOADING) {
        logger.debug(`Waiting for download of LanguageModel to complete.`);
        await this.downloadPromise;
        return true;
      }
      return true;
    }
    if (availability !== Availability.AVAILABLE) {
      logger.debug(`On-device inference unavailable because availability is "${availability}".`);
      return false;
    }
    if (!_ChromeAdapterImpl.isOnDeviceRequest(request)) {
      logger.debug(`On-device inference unavailable because request is incompatible.`);
      return false;
    }
    return true;
  }
  /**
   * Generates content on device.
   *
   * @remarks
   * This is comparable to {@link GenerativeModel.generateContent} for generating content in
   * Cloud.
   * @param request - a standard Firebase AI {@link GenerateContentRequest}
   * @returns {@link Response}, so we can reuse common response formatting.
   */
  async generateContent(request) {
    const session = await this.createSession();
    const contents = await Promise.all(request.contents.map(_ChromeAdapterImpl.toLanguageModelMessage));
    const text = await session.prompt(contents, this.onDeviceParams.promptOptions);
    return _ChromeAdapterImpl.toResponse(text);
  }
  /**
   * Generates content stream on device.
   *
   * @remarks
   * This is comparable to {@link GenerativeModel.generateContentStream} for generating content in
   * Cloud.
   * @param request - a standard Firebase AI {@link GenerateContentRequest}
   * @returns {@link Response}, so we can reuse common response formatting.
   */
  async generateContentStream(request) {
    const session = await this.createSession();
    const contents = await Promise.all(request.contents.map(_ChromeAdapterImpl.toLanguageModelMessage));
    const stream = session.promptStreaming(contents, this.onDeviceParams.promptOptions);
    return _ChromeAdapterImpl.toStreamResponse(stream);
  }
  async countTokens(_request) {
    throw new AIError(AIErrorCode.REQUEST_ERROR, "Count Tokens is not yet available for on-device model.");
  }
  /**
   * Asserts inference for the given request can be performed by an on-device model.
   */
  static isOnDeviceRequest(request) {
    if (request.contents.length === 0) {
      logger.debug("Empty prompt rejected for on-device inference.");
      return false;
    }
    for (const content of request.contents) {
      if (content.role === "function") {
        logger.debug(`"Function" role rejected for on-device inference.`);
        return false;
      }
      for (const part of content.parts) {
        if (part.inlineData && _ChromeAdapterImpl.SUPPORTED_MIME_TYPES.indexOf(part.inlineData.mimeType) === -1) {
          logger.debug(`Unsupported mime type "${part.inlineData.mimeType}" rejected for on-device inference.`);
          return false;
        }
      }
    }
    return true;
  }
  /**
   * Encapsulates logic to get availability and download a model if one is downloadable.
   */
  async downloadIfAvailable() {
    var _a;
    const availability = await ((_a = this.languageModelProvider) == null ? void 0 : _a.availability(this.onDeviceParams.createOptions));
    if (availability === Availability.DOWNLOADABLE) {
      this.download();
    }
    return availability;
  }
  /**
   * Triggers out-of-band download of an on-device model.
   *
   * Chrome only downloads models as needed. Chrome knows a model is needed when code calls
   * LanguageModel.create.
   *
   * Since Chrome manages the download, the SDK can only avoid redundant download requests by
   * tracking if a download has previously been requested.
   */
  download() {
    var _a;
    if (this.isDownloading) {
      return;
    }
    this.isDownloading = true;
    this.downloadPromise = (_a = this.languageModelProvider) == null ? void 0 : _a.create(this.onDeviceParams.createOptions).finally(() => {
      this.isDownloading = false;
    });
  }
  /**
   * Converts Firebase AI {@link Content} object to a Chrome {@link LanguageModelMessage} object.
   */
  static async toLanguageModelMessage(content) {
    const languageModelMessageContents = await Promise.all(content.parts.map(_ChromeAdapterImpl.toLanguageModelMessageContent));
    return {
      role: _ChromeAdapterImpl.toLanguageModelMessageRole(content.role),
      content: languageModelMessageContents
    };
  }
  /**
   * Converts a Firebase AI Part object to a Chrome LanguageModelMessageContent object.
   */
  static async toLanguageModelMessageContent(part) {
    if (part.text) {
      return {
        type: "text",
        value: part.text
      };
    } else if (part.inlineData) {
      const formattedImageContent = await fetch(`data:${part.inlineData.mimeType};base64,${part.inlineData.data}`);
      const imageBlob = await formattedImageContent.blob();
      const imageBitmap = await createImageBitmap(imageBlob);
      return {
        type: "image",
        value: imageBitmap
      };
    }
    throw new AIError(AIErrorCode.REQUEST_ERROR, `Processing of this Part type is not currently supported.`);
  }
  /**
   * Converts a Firebase AI {@link Role} string to a {@link LanguageModelMessageRole} string.
   */
  static toLanguageModelMessageRole(role) {
    return role === "model" ? "assistant" : "user";
  }
  /**
   * Abstracts Chrome session creation.
   *
   * Chrome uses a multi-turn session for all inference. Firebase AI uses single-turn for all
   * inference. To map the Firebase AI API to Chrome's API, the SDK creates a new session for all
   * inference.
   *
   * Chrome will remove a model from memory if it's no longer in use, so this method ensures a
   * new session is created before an old session is destroyed.
   */
  async createSession() {
    if (!this.languageModelProvider) {
      throw new AIError(AIErrorCode.UNSUPPORTED, "Chrome AI requested for unsupported browser version.");
    }
    const newSession = await this.languageModelProvider.create(this.onDeviceParams.createOptions);
    if (this.oldSession) {
      this.oldSession.destroy();
    }
    this.oldSession = newSession;
    return newSession;
  }
  /**
   * Formats string returned by Chrome as a {@link Response} returned by Firebase AI.
   */
  static toResponse(text) {
    return {
      json: async () => ({
        candidates: [
          {
            content: {
              parts: [{ text }]
            }
          }
        ]
      })
    };
  }
  /**
   * Formats string stream returned by Chrome as SSE returned by Firebase AI.
   */
  static toStreamResponse(stream) {
    const encoder = new TextEncoder();
    return {
      body: stream.pipeThrough(new TransformStream({
        transform(chunk, controller) {
          const json = JSON.stringify({
            candidates: [
              {
                content: {
                  role: "model",
                  parts: [{ text: chunk }]
                }
              }
            ]
          });
          controller.enqueue(encoder.encode(`data: ${json}

`));
        }
      }))
    };
  }
};
ChromeAdapterImpl.SUPPORTED_MIME_TYPES = ["image/jpeg", "image/png"];
function chromeAdapterFactory(mode, window2, params) {
  if (typeof window2 !== "undefined" && mode) {
    return new ChromeAdapterImpl(window2.LanguageModel, mode, params);
  }
}
var AIService = class {
  constructor(app, backend, authProvider, appCheckProvider, chromeAdapterFactory2) {
    this.app = app;
    this.backend = backend;
    this.chromeAdapterFactory = chromeAdapterFactory2;
    const appCheck = appCheckProvider == null ? void 0 : appCheckProvider.getImmediate({ optional: true });
    const auth = authProvider == null ? void 0 : authProvider.getImmediate({ optional: true });
    this.auth = auth || null;
    this.appCheck = appCheck || null;
    if (backend instanceof VertexAIBackend) {
      this.location = backend.location;
    } else {
      this.location = "";
    }
  }
  _delete() {
    return Promise.resolve();
  }
  set options(optionsToSet) {
    this._options = optionsToSet;
  }
  get options() {
    return this._options;
  }
};
function factory(container, { instanceIdentifier }) {
  if (!instanceIdentifier) {
    throw new AIError(AIErrorCode.ERROR, "AIService instance identifier is undefined.");
  }
  const backend = decodeInstanceIdentifier(instanceIdentifier);
  const app = container.getProvider("app").getImmediate();
  const auth = container.getProvider("auth-internal");
  const appCheckProvider = container.getProvider("app-check-internal");
  return new AIService(app, backend, auth, appCheckProvider, chromeAdapterFactory);
}
function initApiSettings(ai) {
  var _a, _b, _c, _d, _e, _f, _g;
  if (!((_b = (_a = ai.app) == null ? void 0 : _a.options) == null ? void 0 : _b.apiKey)) {
    throw new AIError(AIErrorCode.NO_API_KEY, `The "apiKey" field is empty in the local Firebase config. Firebase AI requires this field to contain a valid API key.`);
  } else if (!((_d = (_c = ai.app) == null ? void 0 : _c.options) == null ? void 0 : _d.projectId)) {
    throw new AIError(AIErrorCode.NO_PROJECT_ID, `The "projectId" field is empty in the local Firebase config. Firebase AI requires this field to contain a valid project ID.`);
  } else if (!((_f = (_e = ai.app) == null ? void 0 : _e.options) == null ? void 0 : _f.appId)) {
    throw new AIError(AIErrorCode.NO_APP_ID, `The "appId" field is empty in the local Firebase config. Firebase AI requires this field to contain a valid app ID.`);
  }
  const apiSettings = {
    apiKey: ai.app.options.apiKey,
    project: ai.app.options.projectId,
    appId: ai.app.options.appId,
    automaticDataCollectionEnabled: ai.app.automaticDataCollectionEnabled,
    location: ai.location,
    backend: ai.backend
  };
  if (_isFirebaseServerApp(ai.app) && ai.app.settings.appCheckToken) {
    const token = ai.app.settings.appCheckToken;
    apiSettings.getAppCheckToken = () => {
      return Promise.resolve({ token });
    };
  } else if (ai.appCheck) {
    if ((_g = ai.options) == null ? void 0 : _g.useLimitedUseAppCheckTokens) {
      apiSettings.getAppCheckToken = () => ai.appCheck.getLimitedUseToken();
    } else {
      apiSettings.getAppCheckToken = () => ai.appCheck.getToken();
    }
  }
  if (ai.auth) {
    apiSettings.getAuthToken = () => ai.auth.getToken();
  }
  return apiSettings;
}
var AIModel = class _AIModel {
  /**
   * Constructs a new instance of the {@link AIModel} class.
   *
   * This constructor should only be called from subclasses that provide
   * a model API.
   *
   * @param ai - an {@link AI} instance.
   * @param modelName - The name of the model being used. It can be in one of the following formats:
   * - `my-model` (short name, will resolve to `publishers/google/models/my-model`)
   * - `models/my-model` (will resolve to `publishers/google/models/my-model`)
   * - `publishers/my-publisher/models/my-model` (fully qualified model name)
   *
   * @throws If the `apiKey` or `projectId` fields are missing in your
   * Firebase config.
   *
   * @internal
   */
  constructor(ai, modelName) {
    this._apiSettings = initApiSettings(ai);
    this.model = _AIModel.normalizeModelName(modelName, this._apiSettings.backend.backendType);
  }
  /**
   * Normalizes the given model name to a fully qualified model resource name.
   *
   * @param modelName - The model name to normalize.
   * @returns The fully qualified model resource name.
   *
   * @internal
   */
  static normalizeModelName(modelName, backendType) {
    if (backendType === BackendType.GOOGLE_AI) {
      return _AIModel.normalizeGoogleAIModelName(modelName);
    } else {
      return _AIModel.normalizeVertexAIModelName(modelName);
    }
  }
  /**
   * @internal
   */
  static normalizeGoogleAIModelName(modelName) {
    return `models/${modelName}`;
  }
  /**
   * @internal
   */
  static normalizeVertexAIModelName(modelName) {
    let model;
    if (modelName.includes("/")) {
      if (modelName.startsWith("models/")) {
        model = `publishers/google/${modelName}`;
      } else {
        model = modelName;
      }
    } else {
      model = `publishers/google/models/${modelName}`;
    }
    return model;
  }
};
var RequestURL = class {
  constructor(params) {
    this.params = params;
  }
  toString() {
    const url = new URL(this.baseUrl);
    url.pathname = this.pathname;
    url.search = this.queryParams.toString();
    return url.toString();
  }
  get pathname() {
    if (this.params.templateId) {
      return `${this.params.apiSettings.backend._getTemplatePath(this.params.apiSettings.project, this.params.templateId)}:${this.params.task}`;
    } else {
      return `${this.params.apiSettings.backend._getModelPath(this.params.apiSettings.project, this.params.model)}:${this.params.task}`;
    }
  }
  get baseUrl() {
    var _a;
    return ((_a = this.params.requestOptions) == null ? void 0 : _a.baseUrl) ?? `https://${DEFAULT_DOMAIN}`;
  }
  get queryParams() {
    const params = new URLSearchParams();
    if (this.params.stream) {
      params.set("alt", "sse");
    }
    return params;
  }
};
var WebSocketUrl = class {
  constructor(apiSettings) {
    this.apiSettings = apiSettings;
  }
  toString() {
    const url = new URL(`wss://${DEFAULT_DOMAIN}`);
    url.pathname = this.pathname;
    const queryParams = new URLSearchParams();
    queryParams.set("key", this.apiSettings.apiKey);
    url.search = queryParams.toString();
    return url.toString();
  }
  get pathname() {
    if (this.apiSettings.backend.backendType === BackendType.GOOGLE_AI) {
      return "ws/google.firebase.vertexai.v1beta.GenerativeService/BidiGenerateContent";
    } else {
      return `ws/google.firebase.vertexai.v1beta.LlmBidiService/BidiGenerateContent/locations/${this.apiSettings.location}`;
    }
  }
};
function getClientHeaders() {
  const loggingTags = [];
  loggingTags.push(`${LANGUAGE_TAG}/${PACKAGE_VERSION}`);
  loggingTags.push(`fire/${PACKAGE_VERSION}`);
  return loggingTags.join(" ");
}
async function getHeaders(url) {
  const headers = new Headers();
  headers.append("Content-Type", "application/json");
  headers.append("x-goog-api-client", getClientHeaders());
  headers.append("x-goog-api-key", url.params.apiSettings.apiKey);
  if (url.params.apiSettings.automaticDataCollectionEnabled) {
    headers.append("X-Firebase-Appid", url.params.apiSettings.appId);
  }
  if (url.params.apiSettings.getAppCheckToken) {
    const appCheckToken = await url.params.apiSettings.getAppCheckToken();
    if (appCheckToken) {
      headers.append("X-Firebase-AppCheck", appCheckToken.token);
      if (appCheckToken.error) {
        logger.warn(`Unable to obtain a valid App Check token: ${appCheckToken.error.message}`);
      }
    }
  }
  if (url.params.apiSettings.getAuthToken) {
    const authToken = await url.params.apiSettings.getAuthToken();
    if (authToken) {
      headers.append("Authorization", `Firebase ${authToken.accessToken}`);
    }
  }
  return headers;
}
async function makeRequest(requestUrlParams, body) {
  var _a;
  const url = new RequestURL(requestUrlParams);
  let response;
  let fetchTimeoutId;
  try {
    const fetchOptions = {
      method: "POST",
      headers: await getHeaders(url),
      body
    };
    const timeoutMillis = ((_a = requestUrlParams.requestOptions) == null ? void 0 : _a.timeout) != null && requestUrlParams.requestOptions.timeout >= 0 ? requestUrlParams.requestOptions.timeout : DEFAULT_FETCH_TIMEOUT_MS;
    const abortController = new AbortController();
    fetchTimeoutId = setTimeout(() => abortController.abort(), timeoutMillis);
    fetchOptions.signal = abortController.signal;
    response = await fetch(url.toString(), fetchOptions);
    if (!response.ok) {
      let message = "";
      let errorDetails;
      try {
        const json = await response.json();
        message = json.error.message;
        if (json.error.details) {
          message += ` ${JSON.stringify(json.error.details)}`;
          errorDetails = json.error.details;
        }
      } catch (e) {
      }
      if (response.status === 403 && errorDetails && errorDetails.some((detail) => detail.reason === "SERVICE_DISABLED") && errorDetails.some((detail) => {
        var _a2, _b;
        return (_b = (_a2 = detail.links) == null ? void 0 : _a2[0]) == null ? void 0 : _b.description.includes("Google developers console API activation");
      })) {
        throw new AIError(AIErrorCode.API_NOT_ENABLED, `The Firebase AI SDK requires the Firebase AI API ('firebasevertexai.googleapis.com') to be enabled in your Firebase project. Enable this API by visiting the Firebase Console at https://console.firebase.google.com/project/${url.params.apiSettings.project}/genai/ and clicking "Get started". If you enabled this API recently, wait a few minutes for the action to propagate to our systems and then retry.`, {
          status: response.status,
          statusText: response.statusText,
          errorDetails
        });
      }
      throw new AIError(AIErrorCode.FETCH_ERROR, `Error fetching from ${url}: [${response.status} ${response.statusText}] ${message}`, {
        status: response.status,
        statusText: response.statusText,
        errorDetails
      });
    }
  } catch (e) {
    let err = e;
    if (e.code !== AIErrorCode.FETCH_ERROR && e.code !== AIErrorCode.API_NOT_ENABLED && e instanceof Error) {
      err = new AIError(AIErrorCode.ERROR, `Error fetching from ${url.toString()}: ${e.message}`);
      err.stack = e.stack;
    }
    throw err;
  } finally {
    if (fetchTimeoutId) {
      clearTimeout(fetchTimeoutId);
    }
  }
  return response;
}
function hasValidCandidates(response) {
  if (response.candidates && response.candidates.length > 0) {
    if (response.candidates.length > 1) {
      logger.warn(`This response had ${response.candidates.length} candidates. Returning text from the first candidate only. Access response.candidates directly to use the other candidates.`);
    }
    if (hadBadFinishReason(response.candidates[0])) {
      throw new AIError(AIErrorCode.RESPONSE_ERROR, `Response error: ${formatBlockErrorMessage(response)}. Response body stored in error.response`, {
        response
      });
    }
    return true;
  } else {
    return false;
  }
}
function createEnhancedContentResponse(response, inferenceSource = InferenceSource.IN_CLOUD) {
  if (response.candidates && !response.candidates[0].hasOwnProperty("index")) {
    response.candidates[0].index = 0;
  }
  const responseWithHelpers = addHelpers(response);
  responseWithHelpers.inferenceSource = inferenceSource;
  return responseWithHelpers;
}
function addHelpers(response) {
  response.text = () => {
    if (hasValidCandidates(response)) {
      return getText(response, (part) => !part.thought);
    } else if (response.promptFeedback) {
      throw new AIError(AIErrorCode.RESPONSE_ERROR, `Text not available. ${formatBlockErrorMessage(response)}`, {
        response
      });
    }
    return "";
  };
  response.thoughtSummary = () => {
    if (hasValidCandidates(response)) {
      const result = getText(response, (part) => !!part.thought);
      return result === "" ? void 0 : result;
    } else if (response.promptFeedback) {
      throw new AIError(AIErrorCode.RESPONSE_ERROR, `Thought summary not available. ${formatBlockErrorMessage(response)}`, {
        response
      });
    }
    return void 0;
  };
  response.inlineDataParts = () => {
    if (hasValidCandidates(response)) {
      return getInlineDataParts(response);
    } else if (response.promptFeedback) {
      throw new AIError(AIErrorCode.RESPONSE_ERROR, `Data not available. ${formatBlockErrorMessage(response)}`, {
        response
      });
    }
    return void 0;
  };
  response.functionCalls = () => {
    if (hasValidCandidates(response)) {
      return getFunctionCalls(response);
    } else if (response.promptFeedback) {
      throw new AIError(AIErrorCode.RESPONSE_ERROR, `Function call not available. ${formatBlockErrorMessage(response)}`, {
        response
      });
    }
    return void 0;
  };
  return response;
}
function getText(response, partFilter) {
  var _a, _b, _c, _d;
  const textStrings = [];
  if ((_b = (_a = response.candidates) == null ? void 0 : _a[0].content) == null ? void 0 : _b.parts) {
    for (const part of (_d = (_c = response.candidates) == null ? void 0 : _c[0].content) == null ? void 0 : _d.parts) {
      if (part.text && partFilter(part)) {
        textStrings.push(part.text);
      }
    }
  }
  if (textStrings.length > 0) {
    return textStrings.join("");
  } else {
    return "";
  }
}
function getFunctionCalls(response) {
  var _a, _b, _c, _d;
  const functionCalls = [];
  if ((_b = (_a = response.candidates) == null ? void 0 : _a[0].content) == null ? void 0 : _b.parts) {
    for (const part of (_d = (_c = response.candidates) == null ? void 0 : _c[0].content) == null ? void 0 : _d.parts) {
      if (part.functionCall) {
        functionCalls.push(part.functionCall);
      }
    }
  }
  if (functionCalls.length > 0) {
    return functionCalls;
  } else {
    return void 0;
  }
}
function getInlineDataParts(response) {
  var _a, _b, _c, _d;
  const data = [];
  if ((_b = (_a = response.candidates) == null ? void 0 : _a[0].content) == null ? void 0 : _b.parts) {
    for (const part of (_d = (_c = response.candidates) == null ? void 0 : _c[0].content) == null ? void 0 : _d.parts) {
      if (part.inlineData) {
        data.push(part);
      }
    }
  }
  if (data.length > 0) {
    return data;
  } else {
    return void 0;
  }
}
var badFinishReasons = [FinishReason.RECITATION, FinishReason.SAFETY];
function hadBadFinishReason(candidate) {
  return !!candidate.finishReason && badFinishReasons.some((reason) => reason === candidate.finishReason);
}
function formatBlockErrorMessage(response) {
  var _a, _b, _c;
  let message = "";
  if ((!response.candidates || response.candidates.length === 0) && response.promptFeedback) {
    message += "Response was blocked";
    if ((_a = response.promptFeedback) == null ? void 0 : _a.blockReason) {
      message += ` due to ${response.promptFeedback.blockReason}`;
    }
    if ((_b = response.promptFeedback) == null ? void 0 : _b.blockReasonMessage) {
      message += `: ${response.promptFeedback.blockReasonMessage}`;
    }
  } else if ((_c = response.candidates) == null ? void 0 : _c[0]) {
    const firstCandidate = response.candidates[0];
    if (hadBadFinishReason(firstCandidate)) {
      message += `Candidate was blocked due to ${firstCandidate.finishReason}`;
      if (firstCandidate.finishMessage) {
        message += `: ${firstCandidate.finishMessage}`;
      }
    }
  }
  return message;
}
async function handlePredictResponse(response) {
  var _a;
  const responseJson = await response.json();
  const images = [];
  let filteredReason = void 0;
  if (!responseJson.predictions || ((_a = responseJson.predictions) == null ? void 0 : _a.length) === 0) {
    throw new AIError(AIErrorCode.RESPONSE_ERROR, "No predictions or filtered reason received from Vertex AI. Please report this issue with the full error details at https://github.com/firebase/firebase-js-sdk/issues.");
  }
  for (const prediction of responseJson.predictions) {
    if (prediction.raiFilteredReason) {
      filteredReason = prediction.raiFilteredReason;
    } else if (prediction.mimeType && prediction.bytesBase64Encoded) {
      images.push({
        mimeType: prediction.mimeType,
        bytesBase64Encoded: prediction.bytesBase64Encoded
      });
    } else if (prediction.mimeType && prediction.gcsUri) {
      images.push({
        mimeType: prediction.mimeType,
        gcsURI: prediction.gcsUri
      });
    } else if (prediction.safetyAttributes) ;
    else {
      throw new AIError(AIErrorCode.RESPONSE_ERROR, `Unexpected element in 'predictions' array in response: '${JSON.stringify(prediction)}'`);
    }
  }
  return { images, filteredReason };
}
function mapGenerateContentRequest(generateContentRequest) {
  var _a, _b;
  (_a = generateContentRequest.safetySettings) == null ? void 0 : _a.forEach((safetySetting) => {
    if (safetySetting.method) {
      throw new AIError(AIErrorCode.UNSUPPORTED, "SafetySetting.method is not supported in the the Gemini Developer API. Please remove this property.");
    }
  });
  if ((_b = generateContentRequest.generationConfig) == null ? void 0 : _b.topK) {
    const roundedTopK = Math.round(generateContentRequest.generationConfig.topK);
    if (roundedTopK !== generateContentRequest.generationConfig.topK) {
      logger.warn("topK in GenerationConfig has been rounded to the nearest integer to match the format for requests to the Gemini Developer API.");
      generateContentRequest.generationConfig.topK = roundedTopK;
    }
  }
  return generateContentRequest;
}
function mapGenerateContentResponse(googleAIResponse) {
  const generateContentResponse = {
    candidates: googleAIResponse.candidates ? mapGenerateContentCandidates(googleAIResponse.candidates) : void 0,
    prompt: googleAIResponse.promptFeedback ? mapPromptFeedback(googleAIResponse.promptFeedback) : void 0,
    usageMetadata: googleAIResponse.usageMetadata
  };
  return generateContentResponse;
}
function mapCountTokensRequest(countTokensRequest, model) {
  const mappedCountTokensRequest = {
    generateContentRequest: {
      model,
      ...countTokensRequest
    }
  };
  return mappedCountTokensRequest;
}
function mapGenerateContentCandidates(candidates) {
  const mappedCandidates = [];
  let mappedSafetyRatings;
  if (mappedCandidates) {
    candidates.forEach((candidate) => {
      var _a, _b;
      let citationMetadata;
      if (candidate.citationMetadata) {
        citationMetadata = {
          citations: candidate.citationMetadata.citationSources
        };
      }
      if (candidate.safetyRatings) {
        mappedSafetyRatings = candidate.safetyRatings.map((safetyRating) => {
          return {
            ...safetyRating,
            severity: safetyRating.severity ?? HarmSeverity.HARM_SEVERITY_UNSUPPORTED,
            probabilityScore: safetyRating.probabilityScore ?? 0,
            severityScore: safetyRating.severityScore ?? 0
          };
        });
      }
      if ((_b = (_a = candidate.content) == null ? void 0 : _a.parts) == null ? void 0 : _b.some((part) => part == null ? void 0 : part.videoMetadata)) {
        throw new AIError(AIErrorCode.UNSUPPORTED, "Part.videoMetadata is not supported in the Gemini Developer API. Please remove this property.");
      }
      const mappedCandidate = {
        index: candidate.index,
        content: candidate.content,
        finishReason: candidate.finishReason,
        finishMessage: candidate.finishMessage,
        safetyRatings: mappedSafetyRatings,
        citationMetadata,
        groundingMetadata: candidate.groundingMetadata,
        urlContextMetadata: candidate.urlContextMetadata
      };
      mappedCandidates.push(mappedCandidate);
    });
  }
  return mappedCandidates;
}
function mapPromptFeedback(promptFeedback) {
  const mappedSafetyRatings = [];
  promptFeedback.safetyRatings.forEach((safetyRating) => {
    mappedSafetyRatings.push({
      category: safetyRating.category,
      probability: safetyRating.probability,
      severity: safetyRating.severity ?? HarmSeverity.HARM_SEVERITY_UNSUPPORTED,
      probabilityScore: safetyRating.probabilityScore ?? 0,
      severityScore: safetyRating.severityScore ?? 0,
      blocked: safetyRating.blocked
    });
  });
  const mappedPromptFeedback = {
    blockReason: promptFeedback.blockReason,
    safetyRatings: mappedSafetyRatings,
    blockReasonMessage: promptFeedback.blockReasonMessage
  };
  return mappedPromptFeedback;
}
var responseLineRE = /^data\: (.*)(?:\n\n|\r\r|\r\n\r\n)/;
function processStream(response, apiSettings, inferenceSource) {
  const inputStream = response.body.pipeThrough(new TextDecoderStream("utf8", { fatal: true }));
  const responseStream = getResponseStream(inputStream);
  const [stream1, stream2] = responseStream.tee();
  return {
    stream: generateResponseSequence(stream1, apiSettings, inferenceSource),
    response: getResponsePromise(stream2, apiSettings, inferenceSource)
  };
}
async function getResponsePromise(stream, apiSettings, inferenceSource) {
  const allResponses = [];
  const reader = stream.getReader();
  while (true) {
    const { done, value } = await reader.read();
    if (done) {
      let generateContentResponse = aggregateResponses(allResponses);
      if (apiSettings.backend.backendType === BackendType.GOOGLE_AI) {
        generateContentResponse = mapGenerateContentResponse(generateContentResponse);
      }
      return createEnhancedContentResponse(generateContentResponse, inferenceSource);
    }
    allResponses.push(value);
  }
}
async function* generateResponseSequence(stream, apiSettings, inferenceSource) {
  var _a, _b;
  const reader = stream.getReader();
  while (true) {
    const { value, done } = await reader.read();
    if (done) {
      break;
    }
    let enhancedResponse;
    if (apiSettings.backend.backendType === BackendType.GOOGLE_AI) {
      enhancedResponse = createEnhancedContentResponse(mapGenerateContentResponse(value), inferenceSource);
    } else {
      enhancedResponse = createEnhancedContentResponse(value, inferenceSource);
    }
    const firstCandidate = (_a = enhancedResponse.candidates) == null ? void 0 : _a[0];
    if (!((_b = firstCandidate == null ? void 0 : firstCandidate.content) == null ? void 0 : _b.parts) && !(firstCandidate == null ? void 0 : firstCandidate.finishReason) && !(firstCandidate == null ? void 0 : firstCandidate.citationMetadata) && !(firstCandidate == null ? void 0 : firstCandidate.urlContextMetadata)) {
      continue;
    }
    yield enhancedResponse;
  }
}
function getResponseStream(inputStream) {
  const reader = inputStream.getReader();
  const stream = new ReadableStream({
    start(controller) {
      let currentText = "";
      return pump();
      function pump() {
        return reader.read().then(({ value, done }) => {
          if (done) {
            if (currentText.trim()) {
              controller.error(new AIError(AIErrorCode.PARSE_FAILED, "Failed to parse stream"));
              return;
            }
            controller.close();
            return;
          }
          currentText += value;
          let match = currentText.match(responseLineRE);
          let parsedResponse;
          while (match) {
            try {
              parsedResponse = JSON.parse(match[1]);
            } catch (e) {
              controller.error(new AIError(AIErrorCode.PARSE_FAILED, `Error parsing JSON response: "${match[1]}`));
              return;
            }
            controller.enqueue(parsedResponse);
            currentText = currentText.substring(match[0].length);
            match = currentText.match(responseLineRE);
          }
          return pump();
        });
      }
    }
  });
  return stream;
}
function aggregateResponses(responses) {
  const lastResponse = responses[responses.length - 1];
  const aggregatedResponse = {
    promptFeedback: lastResponse == null ? void 0 : lastResponse.promptFeedback
  };
  for (const response of responses) {
    if (response.candidates) {
      for (const candidate of response.candidates) {
        const i = candidate.index || 0;
        if (!aggregatedResponse.candidates) {
          aggregatedResponse.candidates = [];
        }
        if (!aggregatedResponse.candidates[i]) {
          aggregatedResponse.candidates[i] = {
            index: candidate.index
          };
        }
        aggregatedResponse.candidates[i].citationMetadata = candidate.citationMetadata;
        aggregatedResponse.candidates[i].finishReason = candidate.finishReason;
        aggregatedResponse.candidates[i].finishMessage = candidate.finishMessage;
        aggregatedResponse.candidates[i].safetyRatings = candidate.safetyRatings;
        aggregatedResponse.candidates[i].groundingMetadata = candidate.groundingMetadata;
        const urlContextMetadata = candidate.urlContextMetadata;
        if (typeof urlContextMetadata === "object" && urlContextMetadata !== null && Object.keys(urlContextMetadata).length > 0) {
          aggregatedResponse.candidates[i].urlContextMetadata = urlContextMetadata;
        }
        if (candidate.content) {
          if (!candidate.content.parts) {
            continue;
          }
          if (!aggregatedResponse.candidates[i].content) {
            aggregatedResponse.candidates[i].content = {
              role: candidate.content.role || "user",
              parts: []
            };
          }
          for (const part of candidate.content.parts) {
            const newPart = { ...part };
            if (part.text === "") {
              continue;
            }
            if (Object.keys(newPart).length > 0) {
              aggregatedResponse.candidates[i].content.parts.push(newPart);
            }
          }
        }
      }
    }
  }
  return aggregatedResponse;
}
var errorsCausingFallback = [
  // most network errors
  AIErrorCode.FETCH_ERROR,
  // fallback code for all other errors in makeRequest
  AIErrorCode.ERROR,
  // error due to API not being enabled in project
  AIErrorCode.API_NOT_ENABLED
];
async function callCloudOrDevice(request, chromeAdapter, onDeviceCall, inCloudCall) {
  if (!chromeAdapter) {
    return {
      response: await inCloudCall(),
      inferenceSource: InferenceSource.IN_CLOUD
    };
  }
  switch (chromeAdapter.mode) {
    case InferenceMode.ONLY_ON_DEVICE:
      if (await chromeAdapter.isAvailable(request)) {
        return {
          response: await onDeviceCall(),
          inferenceSource: InferenceSource.ON_DEVICE
        };
      }
      throw new AIError(AIErrorCode.UNSUPPORTED, "Inference mode is ONLY_ON_DEVICE, but an on-device model is not available.");
    case InferenceMode.ONLY_IN_CLOUD:
      return {
        response: await inCloudCall(),
        inferenceSource: InferenceSource.IN_CLOUD
      };
    case InferenceMode.PREFER_IN_CLOUD:
      try {
        return {
          response: await inCloudCall(),
          inferenceSource: InferenceSource.IN_CLOUD
        };
      } catch (e) {
        if (e instanceof AIError && errorsCausingFallback.includes(e.code)) {
          return {
            response: await onDeviceCall(),
            inferenceSource: InferenceSource.ON_DEVICE
          };
        }
        throw e;
      }
    case InferenceMode.PREFER_ON_DEVICE:
      if (await chromeAdapter.isAvailable(request)) {
        return {
          response: await onDeviceCall(),
          inferenceSource: InferenceSource.ON_DEVICE
        };
      }
      return {
        response: await inCloudCall(),
        inferenceSource: InferenceSource.IN_CLOUD
      };
    default:
      throw new AIError(AIErrorCode.ERROR, `Unexpected infererence mode: ${chromeAdapter.mode}`);
  }
}
async function generateContentStreamOnCloud(apiSettings, model, params, requestOptions) {
  if (apiSettings.backend.backendType === BackendType.GOOGLE_AI) {
    params = mapGenerateContentRequest(params);
  }
  return makeRequest({
    task: "streamGenerateContent",
    model,
    apiSettings,
    stream: true,
    requestOptions
  }, JSON.stringify(params));
}
async function generateContentStream(apiSettings, model, params, chromeAdapter, requestOptions) {
  const callResult = await callCloudOrDevice(params, chromeAdapter, () => chromeAdapter.generateContentStream(params), () => generateContentStreamOnCloud(apiSettings, model, params, requestOptions));
  return processStream(callResult.response, apiSettings);
}
async function generateContentOnCloud(apiSettings, model, params, requestOptions) {
  if (apiSettings.backend.backendType === BackendType.GOOGLE_AI) {
    params = mapGenerateContentRequest(params);
  }
  return makeRequest({
    model,
    task: "generateContent",
    apiSettings,
    stream: false,
    requestOptions
  }, JSON.stringify(params));
}
async function templateGenerateContent(apiSettings, templateId, templateParams, requestOptions) {
  const response = await makeRequest({
    task: "templateGenerateContent",
    templateId,
    apiSettings,
    stream: false,
    requestOptions
  }, JSON.stringify(templateParams));
  const generateContentResponse = await processGenerateContentResponse(response, apiSettings);
  const enhancedResponse = createEnhancedContentResponse(generateContentResponse);
  return {
    response: enhancedResponse
  };
}
async function templateGenerateContentStream(apiSettings, templateId, templateParams, requestOptions) {
  const response = await makeRequest({
    task: "templateStreamGenerateContent",
    templateId,
    apiSettings,
    stream: true,
    requestOptions
  }, JSON.stringify(templateParams));
  return processStream(response, apiSettings);
}
async function generateContent(apiSettings, model, params, chromeAdapter, requestOptions) {
  const callResult = await callCloudOrDevice(params, chromeAdapter, () => chromeAdapter.generateContent(params), () => generateContentOnCloud(apiSettings, model, params, requestOptions));
  const generateContentResponse = await processGenerateContentResponse(callResult.response, apiSettings);
  const enhancedResponse = createEnhancedContentResponse(generateContentResponse, callResult.inferenceSource);
  return {
    response: enhancedResponse
  };
}
async function processGenerateContentResponse(response, apiSettings) {
  const responseJson = await response.json();
  if (apiSettings.backend.backendType === BackendType.GOOGLE_AI) {
    return mapGenerateContentResponse(responseJson);
  } else {
    return responseJson;
  }
}
function formatSystemInstruction(input) {
  if (input == null) {
    return void 0;
  } else if (typeof input === "string") {
    return { role: "system", parts: [{ text: input }] };
  } else if (input.text) {
    return { role: "system", parts: [input] };
  } else if (input.parts) {
    if (!input.role) {
      return { role: "system", parts: input.parts };
    } else {
      return input;
    }
  }
}
function formatNewContent(request) {
  let newParts = [];
  if (typeof request === "string") {
    newParts = [{ text: request }];
  } else {
    for (const partOrString of request) {
      if (typeof partOrString === "string") {
        newParts.push({ text: partOrString });
      } else {
        newParts.push(partOrString);
      }
    }
  }
  return assignRoleToPartsAndValidateSendMessageRequest(newParts);
}
function assignRoleToPartsAndValidateSendMessageRequest(parts) {
  const userContent = { role: "user", parts: [] };
  const functionContent = { role: "function", parts: [] };
  let hasUserContent = false;
  let hasFunctionContent = false;
  for (const part of parts) {
    if ("functionResponse" in part) {
      functionContent.parts.push(part);
      hasFunctionContent = true;
    } else {
      userContent.parts.push(part);
      hasUserContent = true;
    }
  }
  if (hasUserContent && hasFunctionContent) {
    throw new AIError(AIErrorCode.INVALID_CONTENT, "Within a single message, FunctionResponse cannot be mixed with other type of Part in the request for sending chat message.");
  }
  if (!hasUserContent && !hasFunctionContent) {
    throw new AIError(AIErrorCode.INVALID_CONTENT, "No Content is provided for sending chat message.");
  }
  if (hasUserContent) {
    return userContent;
  }
  return functionContent;
}
function formatGenerateContentInput(params) {
  let formattedRequest;
  if (params.contents) {
    formattedRequest = params;
  } else {
    const content = formatNewContent(params);
    formattedRequest = { contents: [content] };
  }
  if (params.systemInstruction) {
    formattedRequest.systemInstruction = formatSystemInstruction(params.systemInstruction);
  }
  return formattedRequest;
}
function createPredictRequestBody(prompt, { gcsURI, imageFormat, addWatermark, numberOfImages = 1, negativePrompt, aspectRatio, safetyFilterLevel, personFilterLevel }) {
  const body = {
    instances: [
      {
        prompt
      }
    ],
    parameters: {
      storageUri: gcsURI,
      negativePrompt,
      sampleCount: numberOfImages,
      aspectRatio,
      outputOptions: imageFormat,
      addWatermark,
      safetyFilterLevel,
      personGeneration: personFilterLevel,
      includeRaiReason: true,
      includeSafetyAttributes: true
    }
  };
  return body;
}
var VALID_PART_FIELDS = [
  "text",
  "inlineData",
  "functionCall",
  "functionResponse",
  "thought",
  "thoughtSignature"
];
var VALID_PARTS_PER_ROLE = {
  user: ["text", "inlineData"],
  function: ["functionResponse"],
  model: ["text", "functionCall", "thought", "thoughtSignature"],
  // System instructions shouldn't be in history anyway.
  system: ["text"]
};
var VALID_PREVIOUS_CONTENT_ROLES = {
  user: ["model"],
  function: ["model"],
  model: ["user", "function"],
  // System instructions shouldn't be in history.
  system: []
};
function validateChatHistory(history) {
  let prevContent = null;
  for (const currContent of history) {
    const { role, parts } = currContent;
    if (!prevContent && role !== "user") {
      throw new AIError(AIErrorCode.INVALID_CONTENT, `First Content should be with role 'user', got ${role}`);
    }
    if (!POSSIBLE_ROLES.includes(role)) {
      throw new AIError(AIErrorCode.INVALID_CONTENT, `Each item should include role field. Got ${role} but valid roles are: ${JSON.stringify(POSSIBLE_ROLES)}`);
    }
    if (!Array.isArray(parts)) {
      throw new AIError(AIErrorCode.INVALID_CONTENT, `Content should have 'parts' property with an array of Parts`);
    }
    if (parts.length === 0) {
      throw new AIError(AIErrorCode.INVALID_CONTENT, `Each Content should have at least one part`);
    }
    const countFields = {
      text: 0,
      inlineData: 0,
      functionCall: 0,
      functionResponse: 0,
      thought: 0,
      thoughtSignature: 0,
      executableCode: 0,
      codeExecutionResult: 0
    };
    for (const part of parts) {
      for (const key of VALID_PART_FIELDS) {
        if (key in part) {
          countFields[key] += 1;
        }
      }
    }
    const validParts = VALID_PARTS_PER_ROLE[role];
    for (const key of VALID_PART_FIELDS) {
      if (!validParts.includes(key) && countFields[key] > 0) {
        throw new AIError(AIErrorCode.INVALID_CONTENT, `Content with role '${role}' can't contain '${key}' part`);
      }
    }
    if (prevContent) {
      const validPreviousContentRoles = VALID_PREVIOUS_CONTENT_ROLES[role];
      if (!validPreviousContentRoles.includes(prevContent.role)) {
        throw new AIError(AIErrorCode.INVALID_CONTENT, `Content with role '${role}' can't follow '${prevContent.role}'. Valid previous roles: ${JSON.stringify(VALID_PREVIOUS_CONTENT_ROLES)}`);
      }
    }
    prevContent = currContent;
  }
}
var SILENT_ERROR = "SILENT_ERROR";
var ChatSession = class {
  constructor(apiSettings, model, chromeAdapter, params, requestOptions) {
    this.model = model;
    this.chromeAdapter = chromeAdapter;
    this.params = params;
    this.requestOptions = requestOptions;
    this._history = [];
    this._sendPromise = Promise.resolve();
    this._apiSettings = apiSettings;
    if (params == null ? void 0 : params.history) {
      validateChatHistory(params.history);
      this._history = params.history;
    }
  }
  /**
   * Gets the chat history so far. Blocked prompts are not added to history.
   * Neither blocked candidates nor the prompts that generated them are added
   * to history.
   */
  async getHistory() {
    await this._sendPromise;
    return this._history;
  }
  /**
   * Sends a chat message and receives a non-streaming
   * {@link GenerateContentResult}
   */
  async sendMessage(request) {
    var _a, _b, _c, _d, _e;
    await this._sendPromise;
    const newContent = formatNewContent(request);
    const generateContentRequest = {
      safetySettings: (_a = this.params) == null ? void 0 : _a.safetySettings,
      generationConfig: (_b = this.params) == null ? void 0 : _b.generationConfig,
      tools: (_c = this.params) == null ? void 0 : _c.tools,
      toolConfig: (_d = this.params) == null ? void 0 : _d.toolConfig,
      systemInstruction: (_e = this.params) == null ? void 0 : _e.systemInstruction,
      contents: [...this._history, newContent]
    };
    let finalResult = {};
    this._sendPromise = this._sendPromise.then(() => generateContent(this._apiSettings, this.model, generateContentRequest, this.chromeAdapter, this.requestOptions)).then((result) => {
      var _a2, _b2;
      if (result.response.candidates && result.response.candidates.length > 0) {
        this._history.push(newContent);
        const responseContent = {
          parts: ((_a2 = result.response.candidates) == null ? void 0 : _a2[0].content.parts) || [],
          // Response seems to come back without a role set.
          role: ((_b2 = result.response.candidates) == null ? void 0 : _b2[0].content.role) || "model"
        };
        this._history.push(responseContent);
      } else {
        const blockErrorMessage = formatBlockErrorMessage(result.response);
        if (blockErrorMessage) {
          logger.warn(`sendMessage() was unsuccessful. ${blockErrorMessage}. Inspect response object for details.`);
        }
      }
      finalResult = result;
    });
    await this._sendPromise;
    return finalResult;
  }
  /**
   * Sends a chat message and receives the response as a
   * {@link GenerateContentStreamResult} containing an iterable stream
   * and a response promise.
   */
  async sendMessageStream(request) {
    var _a, _b, _c, _d, _e;
    await this._sendPromise;
    const newContent = formatNewContent(request);
    const generateContentRequest = {
      safetySettings: (_a = this.params) == null ? void 0 : _a.safetySettings,
      generationConfig: (_b = this.params) == null ? void 0 : _b.generationConfig,
      tools: (_c = this.params) == null ? void 0 : _c.tools,
      toolConfig: (_d = this.params) == null ? void 0 : _d.toolConfig,
      systemInstruction: (_e = this.params) == null ? void 0 : _e.systemInstruction,
      contents: [...this._history, newContent]
    };
    const streamPromise = generateContentStream(this._apiSettings, this.model, generateContentRequest, this.chromeAdapter, this.requestOptions);
    this._sendPromise = this._sendPromise.then(() => streamPromise).catch((_ignored) => {
      throw new Error(SILENT_ERROR);
    }).then((streamResult) => streamResult.response).then((response) => {
      if (response.candidates && response.candidates.length > 0) {
        this._history.push(newContent);
        const responseContent = { ...response.candidates[0].content };
        if (!responseContent.role) {
          responseContent.role = "model";
        }
        this._history.push(responseContent);
      } else {
        const blockErrorMessage = formatBlockErrorMessage(response);
        if (blockErrorMessage) {
          logger.warn(`sendMessageStream() was unsuccessful. ${blockErrorMessage}. Inspect response object for details.`);
        }
      }
    }).catch((e) => {
      if (e.message !== SILENT_ERROR) {
        logger.error(e);
      }
    });
    return streamPromise;
  }
};
async function countTokensOnCloud(apiSettings, model, params, requestOptions) {
  let body = "";
  if (apiSettings.backend.backendType === BackendType.GOOGLE_AI) {
    const mappedParams = mapCountTokensRequest(params, model);
    body = JSON.stringify(mappedParams);
  } else {
    body = JSON.stringify(params);
  }
  const response = await makeRequest({
    model,
    task: "countTokens",
    apiSettings,
    stream: false,
    requestOptions
  }, body);
  return response.json();
}
async function countTokens(apiSettings, model, params, chromeAdapter, requestOptions) {
  if ((chromeAdapter == null ? void 0 : chromeAdapter.mode) === InferenceMode.ONLY_ON_DEVICE) {
    throw new AIError(AIErrorCode.UNSUPPORTED, "countTokens() is not supported for on-device models.");
  }
  return countTokensOnCloud(apiSettings, model, params, requestOptions);
}
var GenerativeModel = class extends AIModel {
  constructor(ai, modelParams, requestOptions, chromeAdapter) {
    super(ai, modelParams.model);
    this.chromeAdapter = chromeAdapter;
    this.generationConfig = modelParams.generationConfig || {};
    this.safetySettings = modelParams.safetySettings || [];
    this.tools = modelParams.tools;
    this.toolConfig = modelParams.toolConfig;
    this.systemInstruction = formatSystemInstruction(modelParams.systemInstruction);
    this.requestOptions = requestOptions || {};
  }
  /**
   * Makes a single non-streaming call to the model
   * and returns an object containing a single {@link GenerateContentResponse}.
   */
  async generateContent(request) {
    const formattedParams = formatGenerateContentInput(request);
    return generateContent(this._apiSettings, this.model, {
      generationConfig: this.generationConfig,
      safetySettings: this.safetySettings,
      tools: this.tools,
      toolConfig: this.toolConfig,
      systemInstruction: this.systemInstruction,
      ...formattedParams
    }, this.chromeAdapter, this.requestOptions);
  }
  /**
   * Makes a single streaming call to the model
   * and returns an object containing an iterable stream that iterates
   * over all chunks in the streaming response as well as
   * a promise that returns the final aggregated response.
   */
  async generateContentStream(request) {
    const formattedParams = formatGenerateContentInput(request);
    return generateContentStream(this._apiSettings, this.model, {
      generationConfig: this.generationConfig,
      safetySettings: this.safetySettings,
      tools: this.tools,
      toolConfig: this.toolConfig,
      systemInstruction: this.systemInstruction,
      ...formattedParams
    }, this.chromeAdapter, this.requestOptions);
  }
  /**
   * Gets a new {@link ChatSession} instance which can be used for
   * multi-turn chats.
   */
  startChat(startChatParams) {
    return new ChatSession(this._apiSettings, this.model, this.chromeAdapter, {
      tools: this.tools,
      toolConfig: this.toolConfig,
      systemInstruction: this.systemInstruction,
      generationConfig: this.generationConfig,
      safetySettings: this.safetySettings,
      /**
       * Overrides params inherited from GenerativeModel with those explicitly set in the
       * StartChatParams. For example, if startChatParams.generationConfig is set, it'll override
       * this.generationConfig.
       */
      ...startChatParams
    }, this.requestOptions);
  }
  /**
   * Counts the tokens in the provided request.
   */
  async countTokens(request) {
    const formattedParams = formatGenerateContentInput(request);
    return countTokens(this._apiSettings, this.model, formattedParams, this.chromeAdapter);
  }
};
var LiveSession = class {
  /**
   * @internal
   */
  constructor(webSocketHandler, serverMessages) {
    this.webSocketHandler = webSocketHandler;
    this.serverMessages = serverMessages;
    this.isClosed = false;
    this.inConversation = false;
  }
  /**
   * Sends content to the server.
   *
   * @param request - The message to send to the model.
   * @param turnComplete - Indicates if the turn is complete. Defaults to false.
   * @throws If this session has been closed.
   *
   * @beta
   */
  async send(request, turnComplete = true) {
    if (this.isClosed) {
      throw new AIError(AIErrorCode.REQUEST_ERROR, "This LiveSession has been closed and cannot be used.");
    }
    const newContent = formatNewContent(request);
    const message = {
      clientContent: {
        turns: [newContent],
        turnComplete
      }
    };
    this.webSocketHandler.send(JSON.stringify(message));
  }
  /**
   * Sends text to the server in realtime.
   *
   * @example
   * ```javascript
   * liveSession.sendTextRealtime("Hello, how are you?");
   * ```
   *
   * @param text - The text data to send.
   * @throws If this session has been closed.
   *
   * @beta
   */
  async sendTextRealtime(text) {
    if (this.isClosed) {
      throw new AIError(AIErrorCode.REQUEST_ERROR, "This LiveSession has been closed and cannot be used.");
    }
    const message = {
      realtimeInput: {
        text
      }
    };
    this.webSocketHandler.send(JSON.stringify(message));
  }
  /**
   * Sends audio data to the server in realtime.
   *
   * @remarks The server requires that the audio data is base64-encoded 16-bit PCM at 16kHz
   * little-endian.
   *
   * @example
   * ```javascript
   * // const pcmData = ... base64-encoded 16-bit PCM at 16kHz little-endian.
   * const blob = { mimeType: "audio/pcm", data: pcmData };
   * liveSession.sendAudioRealtime(blob);
   * ```
   *
   * @param blob - The base64-encoded PCM data to send to the server in realtime.
   * @throws If this session has been closed.
   *
   * @beta
   */
  async sendAudioRealtime(blob) {
    if (this.isClosed) {
      throw new AIError(AIErrorCode.REQUEST_ERROR, "This LiveSession has been closed and cannot be used.");
    }
    const message = {
      realtimeInput: {
        audio: blob
      }
    };
    this.webSocketHandler.send(JSON.stringify(message));
  }
  /**
   * Sends video data to the server in realtime.
   *
   * @remarks The server requires that the video is sent as individual video frames at 1 FPS. It
   * is recommended to set `mimeType` to `image/jpeg`.
   *
   * @example
   * ```javascript
   * // const videoFrame = ... base64-encoded JPEG data
   * const blob = { mimeType: "image/jpeg", data: videoFrame };
   * liveSession.sendVideoRealtime(blob);
   * ```
   * @param blob - The base64-encoded video data to send to the server in realtime.
   * @throws If this session has been closed.
   *
   * @beta
   */
  async sendVideoRealtime(blob) {
    if (this.isClosed) {
      throw new AIError(AIErrorCode.REQUEST_ERROR, "This LiveSession has been closed and cannot be used.");
    }
    const message = {
      realtimeInput: {
        video: blob
      }
    };
    this.webSocketHandler.send(JSON.stringify(message));
  }
  /**
   * Sends function responses to the server.
   *
   * @param functionResponses - The function responses to send.
   * @throws If this session has been closed.
   *
   * @beta
   */
  async sendFunctionResponses(functionResponses) {
    if (this.isClosed) {
      throw new AIError(AIErrorCode.REQUEST_ERROR, "This LiveSession has been closed and cannot be used.");
    }
    const message = {
      toolResponse: {
        functionResponses
      }
    };
    this.webSocketHandler.send(JSON.stringify(message));
  }
  /**
   * Yields messages received from the server.
   * This can only be used by one consumer at a time.
   *
   * @returns An `AsyncGenerator` that yields server messages as they arrive.
   * @throws If the session is already closed, or if we receive a response that we don't support.
   *
   * @beta
   */
  async *receive() {
    if (this.isClosed) {
      throw new AIError(AIErrorCode.SESSION_CLOSED, "Cannot read from a Live session that is closed. Try starting a new Live session.");
    }
    for await (const message of this.serverMessages) {
      if (message && typeof message === "object") {
        if (LiveResponseType.SERVER_CONTENT in message) {
          yield {
            type: "serverContent",
            ...message.serverContent
          };
        } else if (LiveResponseType.TOOL_CALL in message) {
          yield {
            type: "toolCall",
            ...message.toolCall
          };
        } else if (LiveResponseType.TOOL_CALL_CANCELLATION in message) {
          yield {
            type: "toolCallCancellation",
            ...message.toolCallCancellation
          };
        } else {
          logger.warn(`Received an unknown message type from the server: ${JSON.stringify(message)}`);
        }
      } else {
        logger.warn(`Received an invalid message from the server: ${JSON.stringify(message)}`);
      }
    }
  }
  /**
   * Closes this session.
   * All methods on this session will throw an error once this resolves.
   *
   * @beta
   */
  async close() {
    if (!this.isClosed) {
      this.isClosed = true;
      await this.webSocketHandler.close(1e3, "Client closed session.");
    }
  }
  /**
   * Sends realtime input to the server.
   *
   * @deprecated Use `sendTextRealtime()`, `sendAudioRealtime()`, and `sendVideoRealtime()` instead.
   *
   * @param mediaChunks - The media chunks to send.
   * @throws If this session has been closed.
   *
   * @beta
   */
  async sendMediaChunks(mediaChunks) {
    if (this.isClosed) {
      throw new AIError(AIErrorCode.REQUEST_ERROR, "This LiveSession has been closed and cannot be used.");
    }
    mediaChunks.forEach((mediaChunk) => {
      const message = {
        realtimeInput: { mediaChunks: [mediaChunk] }
      };
      this.webSocketHandler.send(JSON.stringify(message));
    });
  }
  /**
   * @deprecated Use `sendTextRealtime()`, `sendAudioRealtime()`, and `sendVideoRealtime()` instead.
   *
   * Sends a stream of {@link GenerativeContentBlob}.
   *
   * @param mediaChunkStream - The stream of {@link GenerativeContentBlob} to send.
   * @throws If this session has been closed.
   *
   * @beta
   */
  async sendMediaStream(mediaChunkStream) {
    if (this.isClosed) {
      throw new AIError(AIErrorCode.REQUEST_ERROR, "This LiveSession has been closed and cannot be used.");
    }
    const reader = mediaChunkStream.getReader();
    while (true) {
      try {
        const { done, value } = await reader.read();
        if (done) {
          break;
        } else if (!value) {
          throw new Error("Missing chunk in reader, but reader is not done.");
        }
        await this.sendMediaChunks([value]);
      } catch (e) {
        const message = e instanceof Error ? e.message : "Error processing media stream.";
        throw new AIError(AIErrorCode.REQUEST_ERROR, message);
      }
    }
  }
};
var LiveGenerativeModel = class extends AIModel {
  /**
   * @internal
   */
  constructor(ai, modelParams, _webSocketHandler) {
    super(ai, modelParams.model);
    this._webSocketHandler = _webSocketHandler;
    this.generationConfig = modelParams.generationConfig || {};
    this.tools = modelParams.tools;
    this.toolConfig = modelParams.toolConfig;
    this.systemInstruction = formatSystemInstruction(modelParams.systemInstruction);
  }
  /**
   * Starts a {@link LiveSession}.
   *
   * @returns A {@link LiveSession}.
   * @throws If the connection failed to be established with the server.
   *
   * @beta
   */
  async connect() {
    const url = new WebSocketUrl(this._apiSettings);
    await this._webSocketHandler.connect(url.toString());
    let fullModelPath;
    if (this._apiSettings.backend.backendType === BackendType.GOOGLE_AI) {
      fullModelPath = `projects/${this._apiSettings.project}/${this.model}`;
    } else {
      fullModelPath = `projects/${this._apiSettings.project}/locations/${this._apiSettings.location}/${this.model}`;
    }
    const { inputAudioTranscription, outputAudioTranscription, ...generationConfig } = this.generationConfig;
    const setupMessage = {
      setup: {
        model: fullModelPath,
        generationConfig,
        tools: this.tools,
        toolConfig: this.toolConfig,
        systemInstruction: this.systemInstruction,
        inputAudioTranscription,
        outputAudioTranscription
      }
    };
    try {
      const serverMessages = this._webSocketHandler.listen();
      this._webSocketHandler.send(JSON.stringify(setupMessage));
      const firstMessage = (await serverMessages.next()).value;
      if (!firstMessage || !(typeof firstMessage === "object") || !("setupComplete" in firstMessage)) {
        await this._webSocketHandler.close(1011, "Handshake failure");
        throw new AIError(AIErrorCode.RESPONSE_ERROR, "Server connection handshake failed. The server did not respond with a setupComplete message.");
      }
      return new LiveSession(this._webSocketHandler, serverMessages);
    } catch (e) {
      await this._webSocketHandler.close();
      throw e;
    }
  }
};
var ImagenModel = class extends AIModel {
  /**
   * Constructs a new instance of the {@link ImagenModel} class.
   *
   * @param ai - an {@link AI} instance.
   * @param modelParams - Parameters to use when making requests to Imagen.
   * @param requestOptions - Additional options to use when making requests.
   *
   * @throws If the `apiKey` or `projectId` fields are missing in your
   * Firebase config.
   */
  constructor(ai, modelParams, requestOptions) {
    const { model, generationConfig, safetySettings } = modelParams;
    super(ai, model);
    this.requestOptions = requestOptions;
    this.generationConfig = generationConfig;
    this.safetySettings = safetySettings;
  }
  /**
   * Generates images using the Imagen model and returns them as
   * base64-encoded strings.
   *
   * @param prompt - A text prompt describing the image(s) to generate.
   * @returns A promise that resolves to an {@link ImagenGenerationResponse}
   * object containing the generated images.
   *
   * @throws If the request to generate images fails. This happens if the
   * prompt is blocked.
   *
   * @remarks
   * If the prompt was not blocked, but one or more of the generated images were filtered, the
   * returned object will have a `filteredReason` property.
   * If all images are filtered, the `images` array will be empty.
   *
   * @public
   */
  async generateImages(prompt) {
    const body = createPredictRequestBody(prompt, {
      ...this.generationConfig,
      ...this.safetySettings
    });
    const response = await makeRequest({
      task: "predict",
      model: this.model,
      apiSettings: this._apiSettings,
      stream: false,
      requestOptions: this.requestOptions
    }, JSON.stringify(body));
    return handlePredictResponse(response);
  }
  /**
   * Generates images to Cloud Storage for Firebase using the Imagen model.
   *
   * @internal This method is temporarily internal.
   *
   * @param prompt - A text prompt describing the image(s) to generate.
   * @param gcsURI - The URI of file stored in a Cloud Storage for Firebase bucket.
   * This should be a directory. For example, `gs://my-bucket/my-directory/`.
   * @returns A promise that resolves to an {@link ImagenGenerationResponse}
   * object containing the URLs of the generated images.
   *
   * @throws If the request fails to generate images fails. This happens if
   * the prompt is blocked.
   *
   * @remarks
   * If the prompt was not blocked, but one or more of the generated images were filtered, the
   * returned object will have a `filteredReason` property.
   * If all images are filtered, the `images` array will be empty.
   */
  async generateImagesGCS(prompt, gcsURI) {
    const body = createPredictRequestBody(prompt, {
      gcsURI,
      ...this.generationConfig,
      ...this.safetySettings
    });
    const response = await makeRequest({
      task: "predict",
      model: this.model,
      apiSettings: this._apiSettings,
      stream: false,
      requestOptions: this.requestOptions
    }, JSON.stringify(body));
    return handlePredictResponse(response);
  }
};
var WebSocketHandlerImpl = class {
  constructor() {
    if (typeof WebSocket === "undefined") {
      throw new AIError(AIErrorCode.UNSUPPORTED, 'The WebSocket API is not available in this environment. The "Live" feature is not supported here. It is supported in modern browser windows, Web Workers with WebSocket support, and Node >= 22.');
    }
  }
  connect(url) {
    return new Promise((resolve, reject) => {
      this.ws = new WebSocket(url);
      this.ws.binaryType = "blob";
      this.ws.addEventListener("open", () => resolve(), { once: true });
      this.ws.addEventListener("error", () => reject(new AIError(AIErrorCode.FETCH_ERROR, `Error event raised on WebSocket`)), { once: true });
      this.ws.addEventListener("close", (closeEvent) => {
        if (closeEvent.reason) {
          logger.warn(`WebSocket connection closed by server. Reason: '${closeEvent.reason}'`);
        }
      });
    });
  }
  send(data) {
    if (!this.ws || this.ws.readyState !== WebSocket.OPEN) {
      throw new AIError(AIErrorCode.REQUEST_ERROR, "WebSocket is not open.");
    }
    this.ws.send(data);
  }
  async *listen() {
    if (!this.ws) {
      throw new AIError(AIErrorCode.REQUEST_ERROR, "WebSocket is not connected.");
    }
    const messageQueue = [];
    const errorQueue = [];
    let resolvePromise = null;
    let isClosed = false;
    const messageListener = async (event) => {
      let data;
      if (event.data instanceof Blob) {
        data = await event.data.text();
      } else if (typeof event.data === "string") {
        data = event.data;
      } else {
        errorQueue.push(new AIError(AIErrorCode.PARSE_FAILED, `Failed to parse WebSocket response. Expected data to be a Blob or string, but was ${typeof event.data}.`));
        if (resolvePromise) {
          resolvePromise();
          resolvePromise = null;
        }
        return;
      }
      try {
        const obj = JSON.parse(data);
        messageQueue.push(obj);
      } catch (e) {
        const err = e;
        errorQueue.push(new AIError(AIErrorCode.PARSE_FAILED, `Error parsing WebSocket message to JSON: ${err.message}`));
      }
      if (resolvePromise) {
        resolvePromise();
        resolvePromise = null;
      }
    };
    const errorListener = () => {
      errorQueue.push(new AIError(AIErrorCode.FETCH_ERROR, "WebSocket connection error."));
      if (resolvePromise) {
        resolvePromise();
        resolvePromise = null;
      }
    };
    const closeListener = (event) => {
      var _a, _b, _c;
      if (event.reason) {
        logger.warn(`WebSocket connection closed by the server with reason: ${event.reason}`);
      }
      isClosed = true;
      if (resolvePromise) {
        resolvePromise();
        resolvePromise = null;
      }
      (_a = this.ws) == null ? void 0 : _a.removeEventListener("message", messageListener);
      (_b = this.ws) == null ? void 0 : _b.removeEventListener("close", closeListener);
      (_c = this.ws) == null ? void 0 : _c.removeEventListener("error", errorListener);
    };
    this.ws.addEventListener("message", messageListener);
    this.ws.addEventListener("close", closeListener);
    this.ws.addEventListener("error", errorListener);
    while (!isClosed) {
      if (errorQueue.length > 0) {
        const error = errorQueue.shift();
        throw error;
      }
      if (messageQueue.length > 0) {
        yield messageQueue.shift();
      } else {
        await new Promise((resolve) => {
          resolvePromise = resolve;
        });
      }
    }
    if (errorQueue.length > 0) {
      const error = errorQueue.shift();
      throw error;
    }
  }
  close(code, reason) {
    return new Promise((resolve) => {
      if (!this.ws) {
        return resolve();
      }
      this.ws.addEventListener("close", () => resolve(), { once: true });
      if (this.ws.readyState === WebSocket.CLOSED || this.ws.readyState === WebSocket.CONNECTING) {
        return resolve();
      }
      if (this.ws.readyState !== WebSocket.CLOSING) {
        this.ws.close(code, reason);
      }
    });
  }
};
var TemplateGenerativeModel = class {
  /**
   * @hideconstructor
   */
  constructor(ai, requestOptions) {
    this.requestOptions = requestOptions || {};
    this._apiSettings = initApiSettings(ai);
  }
  /**
   * Makes a single non-streaming call to the model and returns an object
   * containing a single {@link GenerateContentResponse}.
   *
   * @param templateId - The ID of the server-side template to execute.
   * @param templateVariables - A key-value map of variables to populate the
   * template with.
   *
   * @beta
   */
  async generateContent(templateId, templateVariables) {
    return templateGenerateContent(this._apiSettings, templateId, { inputs: templateVariables }, this.requestOptions);
  }
  /**
   * Makes a single streaming call to the model and returns an object
   * containing an iterable stream that iterates over all chunks in the
   * streaming response as well as a promise that returns the final aggregated
   * response.
   *
   * @param templateId - The ID of the server-side template to execute.
   * @param templateVariables - A key-value map of variables to populate the
   * template with.
   *
   * @beta
   */
  async generateContentStream(templateId, templateVariables) {
    return templateGenerateContentStream(this._apiSettings, templateId, { inputs: templateVariables }, this.requestOptions);
  }
};
var TemplateImagenModel = class {
  /**
   * @hideconstructor
   */
  constructor(ai, requestOptions) {
    this.requestOptions = requestOptions || {};
    this._apiSettings = initApiSettings(ai);
  }
  /**
   * Makes a single call to the model and returns an object containing a single
   * {@link ImagenGenerationResponse}.
   *
   * @param templateId - The ID of the server-side template to execute.
   * @param templateVariables - A key-value map of variables to populate the
   * template with.
   *
   * @beta
   */
  async generateImages(templateId, templateVariables) {
    const response = await makeRequest({
      task: "templatePredict",
      templateId,
      apiSettings: this._apiSettings,
      stream: false,
      requestOptions: this.requestOptions
    }, JSON.stringify({ inputs: templateVariables }));
    return handlePredictResponse(response);
  }
};
var Schema = class {
  constructor(schemaParams) {
    if (!schemaParams.type && !schemaParams.anyOf) {
      throw new AIError(AIErrorCode.INVALID_SCHEMA, "A schema must have either a 'type' or an 'anyOf' array of sub-schemas.");
    }
    for (const paramKey in schemaParams) {
      this[paramKey] = schemaParams[paramKey];
    }
    this.type = schemaParams.type;
    this.format = schemaParams.hasOwnProperty("format") ? schemaParams.format : void 0;
    this.nullable = schemaParams.hasOwnProperty("nullable") ? !!schemaParams.nullable : false;
  }
  /**
   * Defines how this Schema should be serialized as JSON.
   * See https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/stringify#tojson_behavior
   * @internal
   */
  toJSON() {
    const obj = {
      type: this.type
    };
    for (const prop in this) {
      if (this.hasOwnProperty(prop) && this[prop] !== void 0) {
        if (prop !== "required" || this.type === SchemaType.OBJECT) {
          obj[prop] = this[prop];
        }
      }
    }
    return obj;
  }
  static array(arrayParams) {
    return new ArraySchema(arrayParams, arrayParams.items);
  }
  static object(objectParams) {
    return new ObjectSchema(objectParams, objectParams.properties, objectParams.optionalProperties);
  }
  // eslint-disable-next-line id-blacklist
  static string(stringParams) {
    return new StringSchema(stringParams);
  }
  static enumString(stringParams) {
    return new StringSchema(stringParams, stringParams.enum);
  }
  static integer(integerParams) {
    return new IntegerSchema(integerParams);
  }
  // eslint-disable-next-line id-blacklist
  static number(numberParams) {
    return new NumberSchema(numberParams);
  }
  // eslint-disable-next-line id-blacklist
  static boolean(booleanParams) {
    return new BooleanSchema(booleanParams);
  }
  static anyOf(anyOfParams) {
    return new AnyOfSchema(anyOfParams);
  }
};
var IntegerSchema = class extends Schema {
  constructor(schemaParams) {
    super({
      type: SchemaType.INTEGER,
      ...schemaParams
    });
  }
};
var NumberSchema = class extends Schema {
  constructor(schemaParams) {
    super({
      type: SchemaType.NUMBER,
      ...schemaParams
    });
  }
};
var BooleanSchema = class extends Schema {
  constructor(schemaParams) {
    super({
      type: SchemaType.BOOLEAN,
      ...schemaParams
    });
  }
};
var StringSchema = class extends Schema {
  constructor(schemaParams, enumValues) {
    super({
      type: SchemaType.STRING,
      ...schemaParams
    });
    this.enum = enumValues;
  }
  /**
   * @internal
   */
  toJSON() {
    const obj = super.toJSON();
    if (this.enum) {
      obj["enum"] = this.enum;
    }
    return obj;
  }
};
var ArraySchema = class extends Schema {
  constructor(schemaParams, items) {
    super({
      type: SchemaType.ARRAY,
      ...schemaParams
    });
    this.items = items;
  }
  /**
   * @internal
   */
  toJSON() {
    const obj = super.toJSON();
    obj.items = this.items.toJSON();
    return obj;
  }
};
var ObjectSchema = class extends Schema {
  constructor(schemaParams, properties, optionalProperties = []) {
    super({
      type: SchemaType.OBJECT,
      ...schemaParams
    });
    this.properties = properties;
    this.optionalProperties = optionalProperties;
  }
  /**
   * @internal
   */
  toJSON() {
    const obj = super.toJSON();
    obj.properties = { ...this.properties };
    const required = [];
    if (this.optionalProperties) {
      for (const propertyKey of this.optionalProperties) {
        if (!this.properties.hasOwnProperty(propertyKey)) {
          throw new AIError(AIErrorCode.INVALID_SCHEMA, `Property "${propertyKey}" specified in "optionalProperties" does not exist.`);
        }
      }
    }
    for (const propertyKey in this.properties) {
      if (this.properties.hasOwnProperty(propertyKey)) {
        obj.properties[propertyKey] = this.properties[propertyKey].toJSON();
        if (!this.optionalProperties.includes(propertyKey)) {
          required.push(propertyKey);
        }
      }
    }
    if (required.length > 0) {
      obj.required = required;
    }
    delete obj.optionalProperties;
    return obj;
  }
};
var AnyOfSchema = class extends Schema {
  constructor(schemaParams) {
    if (schemaParams.anyOf.length === 0) {
      throw new AIError(AIErrorCode.INVALID_SCHEMA, "The 'anyOf' array must not be empty.");
    }
    super({
      ...schemaParams,
      type: void 0
      // anyOf schemas do not have an explicit type
    });
    this.anyOf = schemaParams.anyOf;
  }
  /**
   * @internal
   */
  toJSON() {
    const obj = super.toJSON();
    if (this.anyOf && Array.isArray(this.anyOf)) {
      obj.anyOf = this.anyOf.map((s) => s.toJSON());
    }
    return obj;
  }
};
var ImagenImageFormat = class {
  constructor() {
    this.mimeType = "image/png";
  }
  /**
   * Creates an {@link ImagenImageFormat} for a JPEG image.
   *
   * @param compressionQuality - The level of compression (a number between 0 and 100).
   * @returns An {@link ImagenImageFormat} object for a JPEG image.
   *
   * @public
   */
  static jpeg(compressionQuality) {
    if (compressionQuality && (compressionQuality < 0 || compressionQuality > 100)) {
      logger.warn(`Invalid JPEG compression quality of ${compressionQuality} specified; the supported range is [0, 100].`);
    }
    return { mimeType: "image/jpeg", compressionQuality };
  }
  /**
   * Creates an {@link ImagenImageFormat} for a PNG image.
   *
   * @returns An {@link ImagenImageFormat} object for a PNG image.
   *
   * @public
   */
  static png() {
    return { mimeType: "image/png" };
  }
};
var SERVER_INPUT_SAMPLE_RATE = 16e3;
var SERVER_OUTPUT_SAMPLE_RATE = 24e3;
var AUDIO_PROCESSOR_NAME = "audio-processor";
var audioProcessorWorkletString = `
  class AudioProcessor extends AudioWorkletProcessor {
    constructor(options) {
      super();
      this.targetSampleRate = options.processorOptions.targetSampleRate;
      // 'sampleRate' is a global variable available inside the AudioWorkletGlobalScope,
      // representing the native sample rate of the AudioContext.
      this.inputSampleRate = sampleRate;
    }

    /**
     * This method is called by the browser's audio engine for each block of audio data.
     * Input is a single input, with a single channel (input[0][0]).
     */
    process(inputs) {
      const input = inputs[0];
      if (input && input.length > 0 && input[0].length > 0) {
        const pcmData = input[0]; // Float32Array of raw audio samples.
        
        // Simple linear interpolation for resampling.
        const resampled = new Float32Array(Math.round(pcmData.length * this.targetSampleRate / this.inputSampleRate));
        const ratio = pcmData.length / resampled.length;
        for (let i = 0; i < resampled.length; i++) {
          resampled[i] = pcmData[Math.floor(i * ratio)];
        }

        // Convert Float32 (-1, 1) samples to Int16 (-32768, 32767)
        const resampledInt16 = new Int16Array(resampled.length);
        for (let i = 0; i < resampled.length; i++) {
          const sample = Math.max(-1, Math.min(1, resampled[i]));
          if (sample < 0) {
            resampledInt16[i] = sample * 32768;
          } else {
            resampledInt16[i] = sample * 32767;
          }
        }
        
        this.port.postMessage(resampledInt16);
      }
      // Return true to keep the processor alive and processing the next audio block.
      return true;
    }
  }

  // Register the processor with a name that can be used to instantiate it from the main thread.
  registerProcessor('${AUDIO_PROCESSOR_NAME}', AudioProcessor);
`;
var AudioConversationRunner = class {
  constructor(liveSession, options, deps) {
    this.liveSession = liveSession;
    this.options = options;
    this.deps = deps;
    this.isStopped = false;
    this.stopDeferred = new Deferred();
    this.playbackQueue = [];
    this.scheduledSources = [];
    this.nextStartTime = 0;
    this.isPlaybackLoopRunning = false;
    this.liveSession.inConversation = true;
    this.receiveLoopPromise = this.runReceiveLoop().finally(() => this.cleanup());
    this.deps.workletNode.port.onmessage = (event) => {
      if (this.isStopped) {
        return;
      }
      const pcm16 = event.data;
      const base64 = btoa(String.fromCharCode.apply(null, Array.from(new Uint8Array(pcm16.buffer))));
      const chunk = {
        mimeType: "audio/pcm",
        data: base64
      };
      void this.liveSession.sendAudioRealtime(chunk);
    };
  }
  /**
   * Stops the conversation and unblocks the main receive loop.
   */
  async stop() {
    if (this.isStopped) {
      return;
    }
    this.isStopped = true;
    this.stopDeferred.resolve();
    await this.receiveLoopPromise;
  }
  /**
   * Cleans up all audio resources (nodes, stream tracks, context) and marks the
   * session as no longer in a conversation.
   */
  cleanup() {
    this.interruptPlayback();
    this.deps.workletNode.port.onmessage = null;
    this.deps.workletNode.disconnect();
    this.deps.sourceNode.disconnect();
    this.deps.mediaStream.getTracks().forEach((track) => track.stop());
    if (this.deps.audioContext.state !== "closed") {
      void this.deps.audioContext.close();
    }
    this.liveSession.inConversation = false;
  }
  /**
   * Adds audio data to the queue and ensures the playback loop is running.
   */
  enqueueAndPlay(audioData) {
    this.playbackQueue.push(audioData);
    void this.processPlaybackQueue();
  }
  /**
   * Stops all current and pending audio playback and clears the queue. This is
   * called when the server indicates the model's speech was interrupted with
   * `LiveServerContent.modelTurn.interrupted`.
   */
  interruptPlayback() {
    [...this.scheduledSources].forEach((source) => source.stop(0));
    this.playbackQueue.length = 0;
    this.nextStartTime = this.deps.audioContext.currentTime;
  }
  /**
   * Processes the playback queue in a loop, scheduling each chunk in a gapless sequence.
   */
  async processPlaybackQueue() {
    if (this.isPlaybackLoopRunning) {
      return;
    }
    this.isPlaybackLoopRunning = true;
    while (this.playbackQueue.length > 0 && !this.isStopped) {
      const pcmRawBuffer = this.playbackQueue.shift();
      try {
        const pcm16 = new Int16Array(pcmRawBuffer);
        const frameCount = pcm16.length;
        const audioBuffer = this.deps.audioContext.createBuffer(1, frameCount, SERVER_OUTPUT_SAMPLE_RATE);
        const channelData = audioBuffer.getChannelData(0);
        for (let i = 0; i < frameCount; i++) {
          channelData[i] = pcm16[i] / 32768;
        }
        const source = this.deps.audioContext.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(this.deps.audioContext.destination);
        this.scheduledSources.push(source);
        source.onended = () => {
          this.scheduledSources = this.scheduledSources.filter((s) => s !== source);
        };
        this.nextStartTime = Math.max(this.deps.audioContext.currentTime, this.nextStartTime);
        source.start(this.nextStartTime);
        this.nextStartTime += audioBuffer.duration;
      } catch (e) {
        logger.error("Error playing audio:", e);
      }
    }
    this.isPlaybackLoopRunning = false;
  }
  /**
   * The main loop that listens for and processes messages from the server.
   */
  async runReceiveLoop() {
    var _a;
    const messageGenerator = this.liveSession.receive();
    while (!this.isStopped) {
      const result = await Promise.race([
        messageGenerator.next(),
        this.stopDeferred.promise
      ]);
      if (this.isStopped || !result || result.done) {
        break;
      }
      const message = result.value;
      if (message.type === "serverContent") {
        const serverContent = message;
        if (serverContent.interrupted) {
          this.interruptPlayback();
        }
        const audioPart = (_a = serverContent.modelTurn) == null ? void 0 : _a.parts.find((part) => {
          var _a2;
          return (_a2 = part.inlineData) == null ? void 0 : _a2.mimeType.startsWith("audio/");
        });
        if (audioPart == null ? void 0 : audioPart.inlineData) {
          const audioData = Uint8Array.from(atob(audioPart.inlineData.data), (c) => c.charCodeAt(0)).buffer;
          this.enqueueAndPlay(audioData);
        }
      } else if (message.type === "toolCall") {
        if (!this.options.functionCallingHandler) {
          logger.warn("Received tool call message, but StartAudioConversationOptions.functionCallingHandler is undefined. Ignoring tool call.");
        } else {
          try {
            const functionResponse = await this.options.functionCallingHandler(message.functionCalls);
            if (!this.isStopped) {
              void this.liveSession.sendFunctionResponses([functionResponse]);
            }
          } catch (e) {
            throw new AIError(AIErrorCode.ERROR, `Function calling handler failed: ${e.message}`);
          }
        }
      }
    }
  }
};
async function startAudioConversation(liveSession, options = {}) {
  if (liveSession.isClosed) {
    throw new AIError(AIErrorCode.SESSION_CLOSED, "Cannot start audio conversation on a closed LiveSession.");
  }
  if (liveSession.inConversation) {
    throw new AIError(AIErrorCode.REQUEST_ERROR, "An audio conversation is already in progress for this session.");
  }
  if (typeof AudioWorkletNode === "undefined" || typeof AudioContext === "undefined" || typeof navigator === "undefined" || !navigator.mediaDevices) {
    throw new AIError(AIErrorCode.UNSUPPORTED, "Audio conversation is not supported in this environment. It requires the Web Audio API and AudioWorklet support.");
  }
  let audioContext;
  try {
    audioContext = new AudioContext();
    if (audioContext.state === "suspended") {
      await audioContext.resume();
    }
    const mediaStream = await navigator.mediaDevices.getUserMedia({
      audio: true
    });
    const workletBlob = new Blob([audioProcessorWorkletString], {
      type: "application/javascript"
    });
    const workletURL = URL.createObjectURL(workletBlob);
    await audioContext.audioWorklet.addModule(workletURL);
    const sourceNode = audioContext.createMediaStreamSource(mediaStream);
    const workletNode = new AudioWorkletNode(audioContext, AUDIO_PROCESSOR_NAME, {
      processorOptions: { targetSampleRate: SERVER_INPUT_SAMPLE_RATE }
    });
    sourceNode.connect(workletNode);
    const runner = new AudioConversationRunner(liveSession, options, {
      audioContext,
      mediaStream,
      sourceNode,
      workletNode
    });
    return { stop: () => runner.stop() };
  } catch (e) {
    if (audioContext && audioContext.state !== "closed") {
      void audioContext.close();
    }
    if (e instanceof AIError || e instanceof DOMException) {
      throw e;
    }
    throw new AIError(AIErrorCode.ERROR, `Failed to initialize audio recording: ${e.message}`);
  }
}
function getAI(app = getApp(), options) {
  app = getModularInstance(app);
  const AIProvider = _getProvider(app, AI_TYPE);
  const backend = (options == null ? void 0 : options.backend) ?? new GoogleAIBackend();
  const finalOptions = {
    useLimitedUseAppCheckTokens: (options == null ? void 0 : options.useLimitedUseAppCheckTokens) ?? false
  };
  const identifier = encodeInstanceIdentifier(backend);
  const aiInstance = AIProvider.getImmediate({
    identifier
  });
  aiInstance.options = finalOptions;
  return aiInstance;
}
function getGenerativeModel(ai, modelParams, requestOptions) {
  var _a;
  const hybridParams = modelParams;
  let inCloudParams;
  if (hybridParams.mode) {
    inCloudParams = hybridParams.inCloudParams || {
      model: DEFAULT_HYBRID_IN_CLOUD_MODEL
    };
  } else {
    inCloudParams = modelParams;
  }
  if (!inCloudParams.model) {
    throw new AIError(AIErrorCode.NO_MODEL, `Must provide a model name. Example: getGenerativeModel({ model: 'my-model-name' })`);
  }
  const chromeAdapter = (_a = ai.chromeAdapterFactory) == null ? void 0 : _a.call(ai, hybridParams.mode, typeof window === "undefined" ? void 0 : window, hybridParams.onDeviceParams);
  return new GenerativeModel(ai, inCloudParams, requestOptions, chromeAdapter);
}
function getImagenModel(ai, modelParams, requestOptions) {
  if (!modelParams.model) {
    throw new AIError(AIErrorCode.NO_MODEL, `Must provide a model name. Example: getImagenModel({ model: 'my-model-name' })`);
  }
  return new ImagenModel(ai, modelParams, requestOptions);
}
function getLiveGenerativeModel(ai, modelParams) {
  if (!modelParams.model) {
    throw new AIError(AIErrorCode.NO_MODEL, `Must provide a model name for getLiveGenerativeModel. Example: getLiveGenerativeModel(ai, { model: 'my-model-name' })`);
  }
  const webSocketHandler = new WebSocketHandlerImpl();
  return new LiveGenerativeModel(ai, modelParams, webSocketHandler);
}
function getTemplateGenerativeModel(ai, requestOptions) {
  return new TemplateGenerativeModel(ai, requestOptions);
}
function getTemplateImagenModel(ai, requestOptions) {
  return new TemplateImagenModel(ai, requestOptions);
}
function registerAI() {
  _registerComponent(new Component(
    AI_TYPE,
    factory,
    "PUBLIC"
    /* ComponentType.PUBLIC */
  ).setMultipleInstances(true));
  registerVersion(name, version);
  registerVersion(name, version, "esm2020");
}
registerAI();
export {
  AIError,
  AIErrorCode,
  AIModel,
  AnyOfSchema,
  ArraySchema,
  Backend,
  BackendType,
  BlockReason,
  BooleanSchema,
  ChatSession,
  FinishReason,
  FunctionCallingMode,
  GenerativeModel,
  GoogleAIBackend,
  HarmBlockMethod,
  HarmBlockThreshold,
  HarmCategory,
  HarmProbability,
  HarmSeverity,
  ImagenAspectRatio,
  ImagenImageFormat,
  ImagenModel,
  ImagenPersonFilterLevel,
  ImagenSafetyFilterLevel,
  InferenceMode,
  InferenceSource,
  IntegerSchema,
  Language,
  LiveGenerativeModel,
  LiveResponseType,
  LiveSession,
  Modality,
  NumberSchema,
  ObjectSchema,
  Outcome,
  POSSIBLE_ROLES,
  ResponseModality,
  Schema,
  SchemaType,
  StringSchema,
  TemplateGenerativeModel,
  TemplateImagenModel,
  URLRetrievalStatus,
  VertexAIBackend,
  getAI,
  getGenerativeModel,
  getImagenModel,
  getLiveGenerativeModel,
  getTemplateGenerativeModel,
  getTemplateImagenModel,
  startAudioConversation
};
/*! Bundled license information:

@firebase/ai/dist/esm/index.esm.js:
  (**
   * @license
   * Copyright 2024 Google LLC
   *
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   *   http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   *)
  (**
   * @license
   * Copyright 2025 Google LLC
   *
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   *   http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   *)
*/
//# sourceMappingURL=firebase_ai.js.map
